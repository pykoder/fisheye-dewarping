# Fisheye Dewarping : Du Memory Leak Ã  6 ImplÃ©mentations ComparÃ©es

*TL;DR : Face Ã  un bug critique en prod, j'ai dÃ» recoder un dewarper fisheye from scratch. Trois ans plus tard, je revisite ce problÃ¨me pour comparer 6 approches diffÃ©rentes - FFmpeg ligne de commande, Python pur, NumPy vectorisÃ©, OpenCV Python/C++, et une lib C++ custom optimisÃ©e.*

---

## L'incident qui a tout lancÃ©

**Janvier 2022.** Notre conteneur Docker plante rÃ©guliÃ¨rement en prod aprÃ¨s quelques heures sur certains sites. Le diagnostic tombe : memory leak dans la bibliothÃ¨que propriÃ©taire qui gÃ¨re le dewarping des camÃ©ras fisheye 360Â°.

Contexte rapide : notre IA analyse des flux vidÃ©o pour dÃ©tecter des gestes. Nos modÃ¨les sont entraÃ®nÃ©s sur des vues plates, pas sur des vues circulaires dÃ©formÃ©es. Sans dewarping, **zÃ©ro dÃ©tection** sur les camÃ©ras fisheye (environ 1% du parc, mais des clients importants).

La lib qui plante ? Fournie par le fabricant des camÃ©ras. On ne peut ni la patcher, ni attendre un correctif.

**Verdict** : on code notre propre version.

Quinze jours de plongÃ©e dans les bouquins de gÃ©omÃ©trie projective, une implÃ©mentation C++ Ã  base de quaternions et projections sphÃ©riques, quelques semaines d'optimisation... et on avait notre solution en prod.

**Trois ans plus tard**, avec un peu de recul et du temps libre, une question me trotte dans la tÃªte : **et si on l'avait fait autrement ?**

Quelle approche aurait Ã©tÃ© la plus efficace selon nos contraintes :
- CamÃ©ras fisheye **fixes** au plafond
- Besoin de **5 vues plates** par frame
- Points de vue des camÃ©ras virtuelles **statiques** (pas de rotation dynamique)
- **Performance critique** : traitement temps rÃ©el de multiples flux vidÃ©o
- QualitÃ© "suffisante" pour la dÃ©tection (pas besoin de perfection photographique)

Cet article compare **6 implÃ©mentations diffÃ©rentes** pour ce cas d'usage prÃ©cis, avec benchmarks Ã  l'appui.

---

## Ce que vous allez dÃ©couvrir

1. **Les bases thÃ©oriques** du dewarping fisheye (version courte, promis)
2. **Six implÃ©mentations** du mÃªme algorithme :
   - FFmpeg en ligne de commande
   - Python natif (boucles, pas de lib)
   - NumPy vectorisÃ©
   - OpenCV fisheye (Python)
   - OpenCV C++ avec bindings
   - Lib C++ custom optimisÃ©e + wrapper Python
3. **Benchmarks comparatifs** : temps d'exÃ©cution, RAM, complexitÃ© de mise en Å“uvre
4. **Recommandations** selon votre contexte

**Important** : Cette comparaison est spÃ©cifique Ã  *notre* cas d'usage (camÃ©ras fixes, vues statiques, perf temps rÃ©el). Vos besoins peuvent diffÃ©rer radicalement - camÃ©ras mobiles, recalibration dynamique, qualitÃ© maximale, etc. Adaptez en consÃ©quence.

---

## Un peu de thÃ©orie (juste ce qu'il faut)

### Le problÃ¨me en image

Une camÃ©ra fisheye 360Â° au plafond capture tout l'espace dans une image circulaire dÃ©formÃ©e :

Pour que nos algorithmes de dÃ©tection puissent bosser, on transforme Ã§a en plusieurs vues plates rectangulaires :

![SchÃ©ma de principe](https://github.com/pykoder/fisheye-dewarping/blob/main/images/schema.png?raw=true)


### Comment Ã§a marche (version simplifiÃ©e)

Le dewarping se dÃ©compose en deux phases :

**Phase 1 : Calcul du mapping (une seule fois au dÃ©marrage)**

On crÃ©e une table de correspondance : pour chaque pixel de nos 5 vues plates de sortie, on calcule quel pixel de l'image fisheye il faut aller chercher.

ConcrÃ¨tement :
1. On projette chaque point de l'image fisheye sur une demi-sphÃ¨re virtuelle centrÃ©e sur la camÃ©ra
2. On dÃ©finit 5 "camÃ©ras virtuelles" avec leurs positions et orientations fixes
3. Pour chaque camÃ©ra virtuelle, on calcule quelle portion de la sphÃ¨re elle "voit"
4. On stocke tout Ã§a dans une lookup table

Cette phase utilise des projections sphÃ©riques (et dans notre cas initial, des quaternions pour les rotations). C'est du calcul assez lourd, mais on ne le fait **qu'une seule fois**.

**Phase 2 : Application du mapping (pour chaque frame vidÃ©o)**

Pour chaque nouvelle image de la camÃ©ra fisheye :
1. On parcourt nos 5 vues de sortie
2. Pour chaque pixel, on regarde dans la lookup table d'oÃ¹ il vient
3. On copie la couleur du pixel source (avec interpolation optionnelle si on veut de la qualitÃ©)

C'est cette phase qu'on doit optimiser Ã  fond - elle tourne en boucle sur chaque frame.

### Note sur le calibrage

Les lentilles fisheye varient d'un modÃ¨le Ã  l'autre. Un calibrage prÃ©cis permet d'obtenir des vues parfaitement rectilignes. Dans notre cas, on s'en passe : nos algos de dÃ©tection tolÃ¨rent de lÃ©gÃ¨res dÃ©formations rÃ©siduelles. Ã‡a simplifie le code et booste les perfs.

---

## 2.1 FFmpeg CLI - La solution rapide et parallÃ¨le

### L'approche

FFmpeg supporte nativement ce type de dewarping via son filtre `v360`. L'implÃ©mentation est directe mais rÃ©vÃ¨le quelques subtilitÃ©s intÃ©ressantes.

### Le code
```bash
#!/bin/bash
# unwarper_ffmpeg.sh

ffmpeg -y -i "fisheye.mp4" \
-vf "crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_1.mp4" \
-vf "rotate=4*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_2.mp4" \
-vf "rotate=3*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_3.mp4" \
-vf "rotate=2*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_4.mp4" \
-vf "rotate=72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_5.mp4"
```

### DÃ©tails d'implÃ©mentation

**ParamÃ¨tres du filtre v360 :**
- `yaw`, `pitch`, `roll` : rotations dÃ©crivant l'orientation de la camÃ©ra fisheye (dome au plafond)
- `v_fov=90` : champ de vision vertical de la vue de sortie
- `w=960:h=960` : rÃ©solution des vues dewarpÃ©es
- `interp=near` : interpolation plus proche voisin (vs `linear` par dÃ©faut)

**Choix d'optimisation :**

1. **Interpolation minimale** : On force `interp=near` au lieu de l'interpolation linÃ©aire par dÃ©faut. La qualitÃ© d'image est lÃ©gÃ¨rement dÃ©gradÃ©e mais les performances sont meilleures. Pour de la dÃ©tection d'objets, c'est largement suffisant.

2. **Rotation de l'image source** : Le contrÃ´le du point de vue via `yaw/pitch/roll` est limitÃ© et ne permet d'obtenir le rÃ©sultat souhaitÃ© que dans une seule direction. On contourne Ã§a en appliquant une rotation prÃ©alable de l'image fisheye (multiples de 72Â° pour couvrir les 360Â°).

3. **Lecture unique du fichier source** : Toutes les vues sont gÃ©nÃ©rÃ©es en un seul passage de FFmpeg (une commande avec 5 outputs). Essentiel pour les perfs.

4. **VidÃ©o de 128 frames** : Les benchmarks utilisent une vidÃ©o suffisamment longue pour que le temps d'application du mapping domine largement le temps de calcul initial du mapping (la phase qui nous intÃ©resse vraiment).

### Benchmark
```
Commande: ./unwarper_ffmpeg.sh ../images/fisheye_pharma_gde_1920.mp4

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              10.97s
CPU time (user+sys):     84.27s
  - User time:           81.63s
  - System time:         2.64s
CPU utilization:        767%
Cores utilises:         ~7.7
Memoire pic:            1761.50 MB (1803772 KB)
Page faults:            349553 minor, 0 major
Context switches:       44239 vol, 28518 invol
Exit status:            0

Speedup parallele:      7.68x
(CPU time / Wall time = 84.27s / 10.97s)
======================================================================
```

**RÃ©sultats :** 11 secondes pour traiter 128 frames et gÃ©nÃ©rer 5 vues. FFmpeg exploite Ã  fond les 8 cores disponibles (~767% d'utilisation CPU), avec un speedup parallÃ¨le de 7.68x. 

L'utilisation mÃ©moire grimpe toutefois Ã  **1.7 GB**, ce qui est significatif.

### Analyse

**âœ… Points forts**
- **Setup immÃ©diat** : une seule commande, aucune lib Ã  installer (FFmpeg suffit)
- **ParallÃ©lisation native** : utilisation optimale du multi-core sans effort
- **Performances brutes excellentes** : 10.97s pour 128 frames Ã— 5 vues = ~85 ms/frame/vue
- **Robuste** : FFmpeg est battle-tested en prod partout
- **Pas de maintenance** : dÃ©pendance externe stable, bugs dÃ©jÃ  corrigÃ©s par la communautÃ©

**âŒ Points faibles**
- **BoÃ®te noire totale** : impossible d'auditer ou modifier l'algo de dewarping
- **FlexibilitÃ© limitÃ©e** : on est coincÃ© avec les paramÃ¨tres exposÃ©s par `v360`
- **Pas intÃ©grable finement** : nÃ©cessite de spawner un process externe, impossible d'appeler directement comme une fonction Python
- **Consommation mÃ©moire Ã©levÃ©e** : 1.7 GB pour une vidÃ©o 1920Ã—1920, potentiellement problÃ©matique Ã  grande Ã©chelle
- **Optimisations limitÃ©es** : on ne peut pas optimiser la phase de mapping spÃ©cifiquement pour notre cas d'usage (camÃ©ras fixes, vues statiques)

### Verdict

FFmpeg est **l'arme idÃ©ale pour un POC rapide** ou quand vous avez besoin d'un rÃ©sultat qui marche *maintenant* sans vous poser de questions. Parfait pour :
- Tester si le dewarping rÃ©sout votre problÃ¨me mÃ©tier
- Scripts one-shot ou batch processing occasionnel
- Situations oÃ¹ la RAM n'est pas une contrainte

**Mais inadaptÃ© si :**
- Vous devez intÃ©grer le dewarping dans un pipeline Python complexe
- Vous voulez optimiser finement (prÃ©-calcul du mapping une fois, rÃ©utilisation)
- La consommation mÃ©moire est critique
- Vous avez besoin de comprendre ou tweaker l'algorithme sous-jacent

Dans notre cas (memory leak de la lib propriÃ©taire), FFmpeg aurait pu Ãªtre une solution de secours acceptable... mais on aurait vite Ã©tÃ© limitÃ©s pour l'optimisation et l'intÃ©gration.

**Code complet :** [github.com/pykoder/fisheye-dewarping](lien-Ã -adapter)

---
## 2.2 Python pur - Comprendre les maths

### L'approche

Maintenant qu'on a vu la solution "boÃ®te noire" avec FFmpeg, plongeons dans les entrailles de l'algorithme. Cette implÃ©mentation en **Python pur** utilise uniquement les bibliothÃ¨ques standard et NumPy pour la manipulation de tableaux, mais **sans aucune vectorisation**.

L'objectif ici n'est pas la performance, mais la **comprÃ©hension**. Chaque Ã©tape mathÃ©matique est explicite, documentÃ©e, comprÃ©hensible. C'est la rÃ©fÃ©rence pÃ©dagogique qui servira de baseline pour toutes les optimisations ultÃ©rieures.

### Le code
```python
#!/usr/bin/env python3
"""
Pure Python Fisheye Unwarper

Cette implÃ©mentation en Python pur du dewarping fisheye utilise uniquement
des bibliothÃ¨ques standard et numpy pour manipuler des tableaux,
sans aucune optimisation vectorielle.
"""

import numpy as np
from PIL import Image
import math
from typing import Tuple, List


def multiply_quaternion(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    Multiply two quaternions using Python primitives.
    
    Args:
        a: First quaternion [w, x, y, z]
        b: Second quaternion [w, x, y, z]
        
    Returns:
        Result quaternion [w, x, y, z]
    """
    w1, x1, y1, z1 = a[0], a[1], a[2], a[3]
    w2, x2, y2, z2 = b[0], b[1], b[2], b[3]
    
    w = w1*w2 - x1*x2 - y1*y2 - z1*z2
    x = w1*x2 + x1*w2 + y1*z2 - z1*y2
    y = w1*y2 - x1*z2 + y1*w2 + z1*x2
    z = w1*z2 + x1*y2 - y1*x2 + z1*w2
    
    return np.array([w, x, y, z], dtype=np.float64)


class PythonDewarper:
    """
    Pure Python implementation of the fisheye dewarper.    
    """
    
    def __init__(self, width: int, height: int, zones: int = 5):
        """
        Initialize dewarper with image dimensions.
        
        Args:
            width: Image width in pixels (1920)
            height: Image height in pixels (1920)
            zones: Number of perspective views to generate (5)
        """
        self.width = width
        self.height = height
        self.output_width = self.width // 2   # 960px
        self.output_height = self.height // 2  # 960px
        self.zones = zones
        
        # Pre-calculate remapping tables for all views (Phase 1)
        self.remap = self._dewarp_mapping()
    
    def _dewarp_mapping(self) -> List[List[List[Tuple[int, int]]]]:
        """
        Create pixel remapping tables for all views.
        
        This is Phase 1: calculating the mapping once at initialization.
        Returns a 3D lookup table: [zone_id][y][x] -> (src_y, src_x)
        """
       
        # Base orientation: yaw=0, pitch=45Â° (looking down at 45Â°)
        yaw = 0 * np.pi / 360.0
        sin_yaw, cos_yaw = np.sin(yaw), np.cos(yaw)
        yaw_q = np.array([cos_yaw, 0.0, sin_yaw, 0.0], dtype=np.float64)
        
        pitch = 45 * np.pi / 360.0             
        sin_pitch, cos_pitch = np.sin(pitch), np.cos(pitch)
        pitch_q = np.array([cos_pitch, sin_pitch, 0.0, 0.0], dtype=np.float64)

        remap = []
        for zone_id in range(self.zones):
            # Rotate view around Z axis to cover 360Â° in N zones
            roll = (360.0 * zone_id / self.zones) * np.pi / 360.0
            sin_roll, cos_roll = np.sin(roll), np.cos(roll)
            roll_q = np.array([cos_roll, 0.0, 0.0, sin_roll], dtype=np.float64)
    
            # Combine rotations: roll Ã— pitch Ã— yaw
            rq = multiply_quaternion(multiply_quaternion(roll_q, pitch_q), yaw_q)
            
            # Calibration constants to match FFmpeg's camera positioning
            expand = 1.269  # Zoom out slightly
            offset = 0.25   # Move camera back from fisheye center
         
            # Build 3Ã—3 spherical projection matrix from quaternion
            m = np.array([[
                    expand * 4.0 * (rq[0]**2 + rq[1]**2 - rq[2]**2 - rq[3]**2) / self.width,
                    expand * 4.0 * (-rq[0] * rq[3] + rq[1] * rq[2] + rq[2] * rq[1] - rq[3] * rq[0]) / self.height,
                    4.0 * (rq[0] * rq[2] + rq[1] * rq[3] + rq[2] * rq[0] + rq[3] * rq[1]) / np.pi
                ],
                [
                    expand * 4.0 * (rq[0] * rq[3] + rq[1] * rq[2] + rq[2] * rq[1] + rq[3] * rq[0]) / self.width,
                    expand * 4.0 * (rq[0]**2 - rq[1]**2 + rq[2]**2 - rq[3]**2) / self.height,
                    4.0 * (-rq[0] * rq[1] - rq[1] * rq[0] + rq[2] * rq[3] + rq[3] * rq[2])  / np.pi
                ],
                [
                    expand * 4.0 * (-rq[0] * rq[2] + rq[1] * rq[3] - rq[2] * rq[0] + rq[3] * rq[1]) / self.width,
                    expand * 4.0 * (rq[0] * rq[1] + rq[1] * rq[0] + rq[2] * rq[3] + rq[3] * rq[2]) / self.height,
                    4.0 * (rq[0]**2 - rq[1]**2 - rq[2]**2 + rq[3]**2)  / np.pi
                ]], dtype=np.float64)

            # Build remapping table for this zone
            offset_width = offset * self.width
            offset_height = offset * self.height

            remap_zone = []
            for j in range(self.output_height):
                line = []
                y = j - offset_height
                for i in range(self.output_width):
                    x = i - offset_width
                    
                    # Apply projection matrix: get 3D point on hemisphere
                    xyz = ((m[0, 0] * x + m[0, 1] * y + m[0, 2]),
                           (m[1, 0] * x + m[1, 1] * y + m[1, 2]),
                           (m[2, 0] * x + m[2, 1] * y + m[2, 2]))
                    
                    # Convert 3D point to fisheye coordinates
                    hs = np.hypot(xyz[0], xyz[1])  # Horizontal distance
                    phi = np.arctan2(hs, xyz[2])    # Angle from zenith
                    
                    # Map to source pixel in fisheye image
                    src_x = int(self.width * (xyz[0] * phi / (np.pi * hs) + 0.5))
                    src_y = int(self.height * (xyz[1] * phi / (np.pi * hs) + 0.5))
                    
                    line.append((src_y, src_x))
                remap_zone.append(line)
            remap.append(remap_zone)

        return remap
    
    def dewarp_frame(self, image: np.ndarray, zone_id: int):
        """
        Apply dewarping transformation to image (Phase 2).
        
        Args:
            image: Input fisheye image as NumPy array (H, W, 3)
            zone_id: Which perspective view to generate (0-4)
        """
        
        remap_table = self.remap[zone_id]
        output_buffer = np.zeros((self.output_height, self.output_width, 3), dtype=np.uint8)

        # Apply lookup table - simple nearest neighbor
        for i in range(self.output_height):
            for j in range(self.output_width):
                try:
                    output_buffer[i, j] = image[remap_table[i][j]]
                except IndexError:
                    output_buffer[i, j] = [0, 0, 0]  # Black for out-of-bounds

        return output_buffer
```

### Explication dÃ©taillÃ©e des maths

Maintenant qu'on a le code, dÃ©cortiquons ce qui se passe sous le capot. C'est ici qu'on va plonger dans les dÃ©tails mathÃ©matiques - on n'y reviendra plus dans les sections suivantes.

#### Pourquoi les quaternions ?

**Petit rappel historique** : Les quaternions ont Ã©tÃ© inventÃ©s en 1843 par William Hamilton pour reprÃ©senter les rotations dans l'espace 3D. Ils ont depuis conquis la 3D, la robotique, l'aÃ©rospatiale et... le dewarping fisheye.

**Pourquoi pas des angles d'Euler classiques ?**

Les angles d'Euler (yaw/pitch/roll) sont intuitifs mais ont plusieurs dÃ©fauts :
- **Gimbal lock** : certaines combinaisons de rotations causent une perte de degrÃ© de libertÃ©
- **Interpolation non-linÃ©aire** : difficile d'interpoler proprement entre deux orientations
- **Calculs plus lourds** : composer des rotations nÃ©cessite 3 multiplications matricielles 3Ã—3

Les quaternions rÃ¨glent tout Ã§a :
- **Pas de gimbal lock** : toutes les orientations sont reprÃ©sentables sans singularitÃ©
- **Composition efficace** : multiplier deux quaternions = 16 multiplications + 12 additions (vs 27 mult + 18 add pour des matrices 3Ã—3)
- **Compacts** : 4 nombres au lieu de 9 (matrice 3Ã—3)
- **Normalisables facilement** : Ã©vite l'accumulation d'erreurs numÃ©riques lors de compositions successives

**Structure d'un quaternion** : `[w, x, y, z]` oÃ¹ :
- `w` : partie scalaire (cosinus de la demi-rotation)
- `(x, y, z)` : partie vectorielle (direction de l'axe de rotation)

Une rotation de Î¸ autour d'un axe unitaire `(ax, ay, az)` s'Ã©crit :
```
q = [cos(Î¸/2), axÂ·sin(Î¸/2), ayÂ·sin(Î¸/2), azÂ·sin(Î¸/2)]
```

#### Les trois rotations

Notre camÃ©ra virtuelle doit pouvoir regarder dans n'importe quelle direction. On dÃ©compose Ã§a en 3 rotations.

**Convention importante** : Pour une camÃ©ra fisheye montÃ©e **au plafond** (regardant vers le bas), les axes sont diffÃ©rents de la convention classique des camÃ©ras horizontales :

**1. Yaw (rotation autour de Y)** : rotation de la camÃ©ra sur son axe optique
```python
yaw_q = [cos(yaw/2), 0, sin(yaw/2), 0]
```
Dans notre cas : `yaw=0Â°` (pas de rotation sur l'axe)

**2. Pitch (rotation autour de X)** : inclinaison du regard (regarder plus ou moins vers le bas)
```python
pitch_q = [cos(pitch/2), sin(pitch/2), 0, 0]
```
Ici : `pitch=45Â°` (on regarde vers le bas Ã  45Â° depuis l'horizontale, idÃ©al pour voir le sol + une portion du plafond)

**3. Roll (rotation autour de Z)** : tourner la tÃªte Ã  gauche/droite, balayer l'horizon
```python
roll_q = [cos(roll/2), 0, 0, sin(roll/2)]
```
C'est cette rotation qu'on fait varier : `roll = 360Â° Ã— zone_id / 5` pour couvrir les 360Â° en 5 vues (0Â°, 72Â°, 144Â°, 216Â°, 288Â°)

**Composition** : On combine les trois rotations en multipliant les quaternions :
```python
rq = roll_q Ã— pitch_q Ã— yaw_q
```

L'ordre compte ! Ici on applique d'abord le yaw, puis le pitch, puis le roll. L'ordre de multiplication des quaternions suit la rÃ¨gle : la rotation la plus Ã  droite est appliquÃ©e en premier.

#### De quaternion Ã  matrice de projection

Le quaternion nous donne l'orientation de la camÃ©ra, mais pour projeter les pixels on a besoin d'une **matrice 3Ã—3 de projection sphÃ©rique**.

Cette matrice transforme un point `(x, y)` de l'image de sortie en un point `(X, Y, Z)` sur la demi-sphÃ¨re virtuelle centrÃ©e sur la camÃ©ra fisheye.

Les formules proviennent de la conversion quaternion â†’ matrice de rotation, adaptÃ©e pour la projection sphÃ©rique fisheye. Chaque coefficient de la matrice `m[i,j]` encode comment les coordonnÃ©es `(x, y)` de l'output contribuent aux coordonnÃ©es `(X, Y, Z)` du point 3D.

Exemple pour `m[0,0]` (premiÃ¨re ligne, premiÃ¨re colonne) :
```python
m[0,0] = expand * 4.0 * (rq[0]Â² + rq[1]Â² - rq[2]Â² - rq[3]Â²) / width
```

Cette formule vient de la conversion standard quaternion â†’ matrice de rotation, avec :
- Un facteur `4.0` pour la normalisation sphÃ©rique
- Division par `width/height` pour passer en coordonnÃ©es normalisÃ©es [-1, 1]
- Multiplication par `expand` (on y revient juste aprÃ¨s)

Les 9 coefficients de la matrice sont calculÃ©s selon ce mÃªme principe, chacun combinant les composantes du quaternion de maniÃ¨re spÃ©cifique pour encoder la rotation 3D complÃ¨te.

#### Les constantes magiques : `expand` et `offset`

Deux constantes empiriques apparaissent dans le code :

**`expand = 1.269`** : Facteur de zoom
- Sans ce facteur, la projection serait trop "serrÃ©e" et on ne verrait qu'une portion limitÃ©e du champ de vision
- Avec `expand=1.269`, on "recule" lÃ©gÃ¨rement la camÃ©ra virtuelle pour capturer un champ de vision plus large
- Cette valeur a Ã©tÃ© ajustÃ©e pour **matcher exactement le rendu de FFmpeg** (qui applique sa propre calibration interne au filtre `v360`)

**`offset = 0.25`** : DÃ©calage du centre de projection
- Par dÃ©faut, la camÃ©ra virtuelle serait exactement au centre du fisheye (point zÃ©nithal, directement sous la camÃ©ra)
- Avec `offset=0.25`, on dÃ©place la camÃ©ra de 25% de l'image vers l'arriÃ¨re
- **Effet visuel** : on voit Ã  la fois le centre de l'image fisheye (la zone directement sous la camÃ©ra) ET une portion du plafond/bords
- Cette position "en retrait" donne une vue plus Ã©quilibrÃ©e et exploitable

Ces deux constantes sont le rÃ©sultat d'un **reverse engineering empirique** de FFmpeg : on a ajustÃ© les valeurs manuellement jusqu'Ã  obtenir exactement les mÃªmes vues de sortie que le filtre `v360` avec les paramÃ¨tres `yaw=0:pitch=45:v_fov=90`. L'objectif Ã©tait d'avoir une baseline de rÃ©fÃ©rence identique Ã  FFmpeg pour comparer Ã©quitablement les performances des diffÃ©rentes implÃ©mentations.

#### Projection sphÃ©rique finale

Une fois qu'on a le point 3D `(X, Y, Z)` sur la demi-sphÃ¨re, on doit le re-projeter dans l'image fisheye source :
```python
hs = sqrt(XÂ² + YÂ²)           # Distance horizontale du point
phi = atan2(hs, Z)            # Angle depuis le zÃ©nith (0 Ã  Ï€/2)

src_x = width Ã— (X Ã— phi / (Ï€ Ã— hs) + 0.5)
src_y = height Ã— (Y Ã— phi / (Ï€ Ã— hs) + 0.5)
```

Cette formule implÃ©mente la **projection Ã©quidistante** (equidistant projection), le modÃ¨le standard pour les objectifs fisheye :
- L'angle `phi` (angle depuis le zÃ©nith) est proportionnel Ã  la distance radiale dans l'image fisheye
- `X / hs` et `Y / hs` donnent la direction azimutale normalisÃ©e
- Le facteur `phi / (Ï€ Ã— hs)` convertit l'angle en distance radiale normalisÃ©e [0, 0.5]
- Le `+0.5` centre l'image (passage de [-0.5, 0.5] Ã  [0, 1])

Ce modÃ¨le Ã©quidistant signifie qu'un objet Ã  45Â° du centre apparaÃ®t Ã  mi-chemin entre le centre et le bord de l'image fisheye, un objet Ã  90Â° est exactement au bord.

#### RÃ©sumÃ© du pipeline complet

Pour chaque pixel `(i, j)` de l'image dewarpÃ©e de sortie :

1. **Appliquer l'offset** : `x = i - offset_width`, `y = j - offset_height`
2. **Projection par matrice** : `(X, Y, Z) = m Ã— (x, y, 1)` â†’ point 3D sur la demi-sphÃ¨re
3. **Calcul angle sphÃ©rique** : `phi = atan2(sqrt(XÂ²+YÂ²), Z)` â†’ angle depuis le zÃ©nith
4. **Projection fisheye inverse** : `(src_x, src_y)` dans l'image source fisheye
5. **Copie du pixel** : `output[i,j] = input[src_y, src_x]` (nearest neighbor)

**Phases distinctes** :
- Ã‰tapes 1-4 = **Phase 1 - Calcul du mapping** (exÃ©cutÃ©e une seule fois Ã  l'initialisation)
- Ã‰tape 5 = **Phase 2 - Application du mapping** (exÃ©cutÃ©e pour chaque frame vidÃ©o)

Cette sÃ©paration est cruciale pour les performances : le calcul du mapping est coÃ»teux mais ne se fait qu'une fois. L'application du mapping est rÃ©pÃ©tÃ©e des milliers de fois et doit Ãªtre ultra-optimisÃ©e - c'est lÃ  que les optimisations vont se concentrer dans les sections suivantes.

### Benchmark
```
ðŸ” Commande: uv run ./unwarper_python.py ../images/fisheye.jpg -r 128

======================================================================
ðŸ“ˆ RÃ‰SULTATS BENCHMARK
======================================================================
â±ï¸  Wall time:              210.61s
âš™ï¸  CPU time (user+sys):     211.16s
    â”œâ”€ User time:           210.91s
    â””â”€ System time:         0.25s
ðŸ”¥ CPU utilization:        100%
ðŸ’» Cores utilisÃ©s:         ~1.0
ðŸ§  MÃ©moire pic:            646.98 MB (662508 KB)
ðŸ“„ Page faults:            172455 minor, 19 major
ðŸ”„ Context switches:       51 vol, 2313 invol
âœ… Exit status:            0
======================================================================

ðŸ’¡ Speedup parallÃ¨le:      1.00x
   (CPU time / Wall time = 211.16s / 210.61s)

```

**RÃ©sultats** : 211 secondes (3 minutes 31 secondes) pour traiter 128 frames Ã— 5 vues. Soit environ **329 ms par frame et par vue**.

### Analyse

**Performance comparÃ©e Ã  FFmpeg** :
- FFmpeg : 11s pour 128 frames Ã— 5 vues â†’ **17 ms/frame/vue**
- Python pur : 211s pour 128 frames Ã— 5 vues â†’ **330 ms/frame/vue**
- **Ratio : Python pur est 20Ã— plus lent que FFmpeg**

**âœ… Points forts**

- **Code lisible et comprÃ©hensible** : 200 lignes de Python clair oÃ¹ chaque Ã©tape mathÃ©matique est explicite. IdÃ©al pour comprendre l'algorithme, le debugger, ou l'adapter Ã  un nouveau cas d'usage.
- **Consommation mÃ©moire modÃ©rÃ©e** : 650 MB vs 1800 MB pour FFmpeg, soit **2.8Ã— moins**. La table de remapping prÃ©-calculÃ©e est compacte (~9 MB pour 5 vues), et on ne charge qu'une frame Ã  la fois.
- **Baseline de rÃ©fÃ©rence solide** : ImplÃ©mentation correcte et vÃ©rifiÃ©e qu'on peut utiliser comme point de comparaison pour toutes les optimisations futures.
- **Facilement modifiable** : Besoin de changer l'angle de vue ? Les paramÃ¨tres de calibration ? Tout est accessible et modifiable sans recompiler quoi que ce soit.
- **Mono-core total** : CPU Ã  100% signifie qu'on utilise **un seul core**. Le GIL (Global Interpreter Lock) de Python empÃªche le parallÃ©lisme. FFmpeg utilisait 7.7 cores en parallÃ¨le, on reste Ã  1. Dans notre cas d'usage on peut considÃ©rer cela comme une qualitÃ©, car les ressources CPU sont utilisÃ©es efficacement. Python ne consomme que 3 fois plus de ressources CPU que ffmpeg pour le mÃªme traitement.


**âŒ Points faibles**

- **Dramatiquement lent** : 20Ã— plus lent que FFmpeg, pas utilisable en production pour du temps rÃ©el.
- **Boucles Python catastrophiques** : On a 960 Ã— 960 Ã— 5 = 4,6 millions d'itÃ©rations de boucles Python par frame. Chaque itÃ©ration implique du bytecode Python interprÃ©tÃ© (accÃ¨s dictionnaire, indexation NumPy, assignation, gestion d'exceptions), ce qui est des ordres de grandeur plus lent que du code natif.
- **Aucune vectorisation** : NumPy est utilisÃ© uniquement comme conteneur de donnÃ©es. On n'exploite **aucune** des optimisations SIMD ou des opÃ©rations vectorielles batch qu'il offre.

### Verdict

Python pur est **20Ã— plus lent** que FFmpeg. C'est un outil **pÃ©dagogique**, pas une solution de production.

**Ce code est parfait pour** :
- Comprendre exactement comment fonctionne le dewarping fisheye
- Servir de rÃ©fÃ©rence pour vÃ©rifier la correction des implÃ©mentations optimisÃ©es
- Prototyper rapidement des variations de l'algorithme (nouveaux angles, calibrations diffÃ©rentes)
- Apprendre les maths derriÃ¨re (quaternions, projections sphÃ©riques)

**InadaptÃ© pour** :
- Production ou temps rÃ©el (20Ã— trop lent, 3x trop consommateur de ressources)
- Traitement de gros volumes de vidÃ©os
- Tout cas d'usage oÃ¹ la performance compte

**La question maintenant** : peut-on garder la simplicitÃ© de Python tout en rattrapant FFmpeg ? La prochaine section explore la vectorisation NumPy - premiÃ¨re Ã©tape vers des performances acceptables sans quitter Python.

**Code complet** : [github.com/pykoder/fisheye-dewarping](lien-Ã -adapter)

## 2.3 NumPy vectorisÃ© - Le premier boost de performance

### L'approche

On garde exactement le mÃªme algorithme que la version Python pur, mais on **Ã©limine toutes les boucles Python** en utilisant les opÃ©rations vectorisÃ©es de NumPy. L'idÃ©e : laisser NumPy (Ã©crit en C optimisÃ©) gÃ©rer les millions d'itÃ©rations au lieu de l'interprÃ©teur Python.

Le calcul du mapping reste identique (quaternions, matrice de projection), mais la phase d'application devient massivement parallÃ¨le grÃ¢ce au broadcasting et Ã  la vectorisation.

### Les modifications clÃ©s

Au lieu d'itÃ©rer pixel par pixel avec des boucles Python imbriquÃ©es, on calcule tout d'un coup en manipulant des tableaux entiers.

#### Phase 1 : Calcul du mapping vectorisÃ©

**Avant (Python pur)** :
```python
remap_zone = []
for j in range(self.output_height):
    line = []
    y = j - offset_height
    for i in range(self.output_width):
        x = i - offset_width
        # Calculs pour ce pixel...
        line.append((src_y, src_x))
    remap_zone.append(line)
```

**AprÃ¨s (NumPy vectorisÃ©)** :
```python
# CrÃ©er une grille de toutes les coordonnÃ©es d'un coup
j_coords, i_coords = np.meshgrid(
    np.arange(self.output_width),
    np.arange(self.output_height), 
    indexing='ij')

# Aplatir et appliquer l'offset
x_coords = i_coords.flatten() - offset_width
y_coords = j_coords.flatten() - offset_height

# Empiler en une matrice de coordonnÃ©es homogÃ¨nes
coords = np.column_stack([x_coords, y_coords, np.ones_like(x_coords)])

# UNE multiplication matricielle pour TOUS les pixels
xyz = coords @ m.T

# Calculs vectorisÃ©s (appliquÃ©s Ã  tous les pixels simultanÃ©ment)
hs = np.hypot(xyz[:, 0], xyz[:, 1])
phi = np.arctan2(hs, xyz[:, 2])

# CoordonnÃ©es source pour tous les pixels
src_x = (self.width * (xyz[:, 0] * phi / (np.pi * hs) + 0.5)).astype(np.int32)
src_y = (self.height * (xyz[:, 1] * phi / (np.pi * hs) + 0.5)).astype(np.int32)

# Clipper pour rester dans les bornes de l'image
src_x = np.clip(src_x, 0, self.width - 1)
src_y = np.clip(src_y, 0, self.height - 1)

# Reshape en 2D et stocker
zone_mapping = np.stack([
    src_x.reshape((self.output_height, self.output_width)),
    src_y.reshape((self.output_height, self.output_width))
], axis=-1)
```

**Gain** : Au lieu de 960Ã—960 = 921,600 itÃ©rations de boucles Python, on a **une seule** multiplication matricielle optimisÃ©e en C + quelques opÃ©rations vectorisÃ©es. NumPy utilise les instructions SIMD du CPU (SSE, AVX) pour traiter plusieurs valeurs simultanÃ©ment.

#### Phase 2 : Application du mapping vectorisÃ©

**Avant (Python pur)** :
```python
for i in range(self.output_height):
    for j in range(self.output_width):
        try:
            output_buffer[i, j] = image[remap_table[i][j]]
        except IndexError:
            output_buffer[i, j] = [0, 0, 0]
```

**AprÃ¨s (NumPy vectorisÃ©)** :
```python
# Extraire les coordonnÃ©es sources
src_x = remap_table[:, :, 0]
src_y = remap_table[:, :, 1]

# Indexation avancÃ©e NumPy : copie TOUS les pixels d'un coup
output_buffer = image[src_y, src_x]
```

**DÃ©tail critique** : Le piÃ¨ge du masque de validitÃ©

Une premiÃ¨re version tentait de gÃ©rer explicitement les pixels hors-limites avec un masque boolÃ©en :
```python
valid_mask = ((src_y >= 0) & (src_y < self.height) &
              (src_x >= 0) & (src_x < self.width))
output_buffer[valid_mask] = image[src_y[valid_mask], src_x[valid_mask]]
```

**Impact dÃ©sastreux** : le benchmark passe de **7.97s Ã  18.58s** ! Pourquoi un simple masque de validitÃ© ralentit-il de 2.3Ã— ?

Le masque boolÃ©en casse la **localitÃ© mÃ©moire**. Sans masque, NumPy accÃ¨de aux pixels de faÃ§on relativement sÃ©quentielle, exploitant les caches CPU. Avec le masque, les accÃ¨s deviennent alÃ©atoires et dispersÃ©s - chaque pixel valide peut Ãªtre n'importe oÃ¹ dans l'image source. Le CPU passe son temps Ã  attendre des donnÃ©es du RAM au lieu de calculer.

**Solution Ã©lÃ©gante** : Clipper les coordonnÃ©es lors du calcul du mapping (Phase 1) :
```python
src_x = np.clip(src_x, 0, self.width - 1)
src_y = np.clip(src_y, 0, self.height - 1)
```

Les quelques pixels qui dÃ©passent pointent maintenant vers le bord de l'image (artefact visuel nÃ©gligeable sur quelques pixels) mais **tous les accÃ¨s mÃ©moire restent valides et sÃ©quentiels**. NumPy peut optimiser agressivement l'indexation.

### Benchmark
```
Commande: python3 unwarper_numpy.py -r 128 ../images/fisheye.jpg

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              7.97s
CPU time (user+sys):     10.69s
  - User time:           10.58s
  - System time:         0.11s
CPU utilization:        134%
Cores utilises:         ~1.3
Memoire pic:            256.19 MB (262336 KB)
======================================================================

Speedup parallele:      1.34x
(CPU time / Wall time = 10.69s / 7.97s)
```

**RÃ©sultats** : 7.97 secondes pour traiter 128 frames Ã— 5 vues. Soit environ **12.5 ms par frame et par vue**.

**Performance comparÃ©e** :
- FFmpeg : 10.97s â†’ **17 ms/frame/vue**
- Python pur : 274.32s â†’ **428 ms/frame/vue**
- NumPy vectorisÃ© : 7.97s â†’ **12.5 ms/frame/vue**

### Analyse

**Gains significatifs** :
- **34Ã— plus rapide** que Python pur (274.32s â†’ 7.97s)
- **1.4Ã— plus rapide** que FFmpeg (10.97s â†’ 7.97s)

La vectorisation NumPy Ã©limine le coÃ»t catastrophique des boucles Python, mais on n'obtient pas les gains astronomiques qu'on aurait pu espÃ©rer. Pourquoi ?

**Analyse du parallÃ©lisme** : CPU utilization Ã  134% signifie qu'on utilise ~1.3 cores. C'est mieux que Python pur (1.0 core) mais **trÃ¨s loin** de FFmpeg (7.7 cores). NumPy parallÃ©lise certaines opÃ©rations (multiplication matricielle, fonctions transcendantes) mais l'indexation avancÃ©e `image[src_y, src_x]` reste largement sÃ©quentielle.

**La magie de NumPy (partielle)** :

âœ… **Code C optimisÃ©** : Les opÃ©rations NumPy sont implÃ©mentÃ©es en C hautement optimisÃ© avec `-O3`.

âœ… **Vectorisation SIMD limitÃ©e** : Les calculs mathÃ©matiques (`hypot`, `arctan2`) exploitent les instructions AVX pour traiter 4-8 float64 simultanÃ©ment. Gain rÃ©el sur la Phase 1.

âœ… **LocalitÃ© mÃ©moire** : Les opÃ©rations vectorisÃ©es accÃ¨dent Ã  la mÃ©moire sÃ©quentiellement, maximisant l'efficacitÃ© du cache (sauf si on utilise le masque de validitÃ© !).

âŒ **ParallÃ©lisme limitÃ©** : NumPy ne parallÃ©lise que les grosses multiplications matricielles. L'indexation avancÃ©e reste mono-thread. Contrairement Ã  FFmpeg qui parallÃ©lise le dÃ©codage vidÃ©o + les filtres sur tous les cores.

âŒ **GIL partiellement prÃ©sent** : Certaines opÃ©rations NumPy relÃ¢chent le GIL, d'autres non. L'indexation avancÃ©e garde souvent le GIL, limitant le parallÃ©lisme.

**âœ… Points forts**

- **Performance acceptable** : 1.4Ã— plus rapide que FFmpeg, 34Ã— plus rapide que Python pur. Utilisable en production pour des volumes modÃ©rÃ©s.
- **Code toujours en Python** : GardÃ© la simplicitÃ© et la lisibilitÃ© de Python. Facile Ã  modifier, debugger, intÃ©grer dans un pipeline existant.
- **Pas de compilation** : Aucun toolchain C++, CMake ou dÃ©pendances systÃ¨me complexes. Juste `pip install numpy` et Ã§a tourne.
- **MÃ©moire optimisÃ©e** : 256 MB vs 638 MB (Python pur) et 1761 MB (FFmpeg). La table de mapping NumPy est compacte (array dense contiguÃ« en mÃ©moire).
- **LÃ©ger multithreading** : ~1.3 cores utilisÃ©s vs 1.0 pour Python pur. NumPy parallÃ©lise automatiquement certaines opÃ©rations.

**âŒ Points faibles**

- **ParallÃ©lisme dÃ©cevant** : 1.3 cores vs 7.7 pour FFmpeg. On n'exploite pas le potentiel multi-core de la machine. L'indexation avancÃ©e reste le goulot mono-thread.
- **Pas de vrai gain sur FFmpeg** : 1.4Ã— plus rapide, c'est bien mais pas spectaculaire. FFmpeg reste compÃ©titif grÃ¢ce Ã  son parallÃ©lisme agressif.
- **DÃ©pendance NumPy** : NÃ©cessite NumPy + ses backends (OpenBLAS ou MKL). Packaging plus lourd qu'un simple script Python, possibles conflits de versions.
- **Courbe d'apprentissage** : Broadcasting, indexation avancÃ©e, piÃ¨ges de performance (masque de validitÃ©) - il faut maÃ®triser ces concepts pour ne pas se tirer une balle dans le pied.
- **Optimisations limitÃ©es** : On ne peut pas tweaker finement les stratÃ©gies d'accÃ¨s mÃ©moire ou le threading. NumPy dÃ©cide pour nous.

### Verdict

NumPy vectorisÃ© apporte un **gain substantiel de 34Ã— sur Python pur**, prouvant que la vectorisation fonctionne. Mais le gain modeste sur FFmpeg (1.4Ã—) rÃ©vÃ¨le les limites de cette approche : **on reste fondamentalement mono-thread** sur la partie critique (indexation).

**Cette implÃ©mentation est adaptÃ©e pour** :
- Pipelines Python existants oÃ¹ ajouter du C++ serait compliquÃ©
- Prototypage rapide avec performances acceptables
- Situations oÃ¹ 1.4Ã— plus rapide que FFmpeg suffit et oÃ¹ la simplicitÃ© de dÃ©ploiement prime

**Limitations** :
- Pour exploiter vraiment le multi-core, il faut sortir de Python
- L'indexation avancÃ©e NumPy ne parallÃ©lise pas bien
- On plafonne Ã  ~1.3 cores quoi qu'on fasse en pur NumPy

**Peut-on faire mieux en restant en Python ?** Oui - la prochaine section explore OpenCV Python, qui offre des fonctions dÃ©diÃ©es au dewarping fisheye avec des optimisations spÃ©cifiques au traitement d'image.

**Code complet** : [github.com/pykoder/fisheye-dewarping](lien-Ã -adapter)

---
### OpenCV : La Puissance du C++

## 2.4 OpenCV Python - La fonction dÃ©diÃ©e cv2.remap()

### L'approche

OpenCV est **la** bibliothÃ¨que de rÃ©fÃ©rence en computer vision. Elle offre une fonction spÃ©cialisÃ©e pour les transformations gÃ©omÃ©triques : `cv2.remap()`, conÃ§ue spÃ©cifiquement pour appliquer des tables de correspondance pixel Ã  pixel.

On garde notre algorithme de mapping (quaternions, projection sphÃ©rique) mais on dÃ©lÃ¨gue la phase d'application Ã  OpenCV. Bonus : on a pu simplifier les formules de projection en utilisant une approche plus classique basÃ©e sur la matrice de camÃ©ra, Ã©liminant les constantes empiriques `expand` et `offset`.

### Les modifications clÃ©s

#### Projection simplifiÃ©e

**Avant (avec constantes magiques)** :
```python
expand = 1.269  # Facteur empirique
offset = 0.25   # Offset empirique
# ... formules complexes avec ces constantes
```

**AprÃ¨s (projection classique)** :
```python
# Matrice de camÃ©ra pour la vue perspective
K = np.array([
    [self.output_width / 2, 0, self.output_width / 2],
    [0, self.output_height / 2, self.output_height / 2],
    [0, 0, 1]
], dtype=np.float32)

# Projection inverse : pixels â†’ rayons 3D
rays = np.linalg.inv(K) @ xyz

# Normalisation et rotation
rays = rays / np.linalg.norm(rays, axis=0, keepdims=True)
rays_fisheye = R @ rays

# Projection fisheye Ã©quidistante
theta = np.arccos(np.clip(rays_fisheye[2, :], -1, 1))
phi = np.arctan2(rays_fisheye[1, :], rays_fisheye[0, :])
r = theta * self.width / np.pi

x = r * np.cos(phi) + self.width / 2
y = r * np.sin(phi) + self.height / 2
```

Formules standard de projection perspective + fisheye Ã©quidistante. Plus besoin de reverse-engineer FFmpeg.

#### Application avec cv2.remap()

**NumPy vectorisÃ©** :
```python
src_x = remap_table[:, :, 0]
src_y = remap_table[:, :, 1]
output_buffer = image[src_y, src_x]
```

**OpenCV** :
```python
map_x = remap_table[:, :, 0].astype(np.float32)
map_y = remap_table[:, :, 1].astype(np.float32)

output = cv2.remap(
    image, 
    map_x, 
    map_y, 
    cv2.INTER_NEAREST,              # Plus proche voisin
    borderMode=cv2.BORDER_CONSTANT,
    borderValue=(0, 0, 0)
)
```

**DiffÃ©rences notables** :

1. **Type `float32` obligatoire** : OpenCV exige des maps en `float32` (vs `int32` pour NumPy). Conversion nÃ©cessaire.

2. **Clipping automatique** : `cv2.remap()` gÃ¨re automatiquement les pixels hors-limite via `borderMode`. On ne peut pas le dÃ©sactiver - OpenCV vÃ©rifie systÃ©matiquement les bornes. Avec `BORDER_CONSTANT`, les pixels hors-limite deviennent noirs.

3. **Interpolation explicite** : `INTER_NEAREST` pour performances maximales. `INTER_LINEAR` ou `INTER_CUBIC` disponibles pour meilleure qualitÃ©.

### Benchmark
```
Commande: python3 unwarper_opencv.py -r 1000 ../images/fisheye.jpg

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              12.35s
CPU time (user+sys):     56.00s
  - User time:           42.85s
  - System time:         13.15s
CPU utilization:        453%
Cores utilises:         ~4.5
Memoire pic:            288.39 MB (295316 KB)
======================================================================

Speedup parallele:      4.53x
(CPU time / Wall time = 56.00s / 12.35s)
```

**Comparaison NumPy** (mÃªme test avec 1000 rÃ©pÃ©titions) :
```
Commande: python3 unwarper_numpy.py -r 1000 ../images/fisheye.jpg

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              56.57s
CPU time (user+sys):     60.09s
CPU utilization:        106%
Cores utilises:         ~1.1
======================================================================

Speedup parallele:      1.06x
```

**RÃ©sultats** : 
- OpenCV : 12.35s wall time, 56.00s CPU time
- NumPy : 56.57s wall time, 60.09s CPU time

### Analyse

**Comparaison des ressources CPU** :

Sur une machine partagÃ©e, ce qui compte c'est le **CPU time total** consommÃ© (charge sur la machine), pas seulement le wall clock time.

- NumPy : **60.09s CPU time** pour traiter 5000 vues (1000 frames Ã— 5 vues)
- OpenCV : **56.00s CPU time** pour le mÃªme travail
- **Gain rÃ©el : 7% de CPU Ã©conomisÃ©** (60.09s â†’ 56.00s)

**Pourquoi si peu de diffÃ©rence en CPU time ?**

Le multithreading d'OpenCV (4.5 cores) **ne rÃ©duit pas la charge CPU totale** - il la distribue juste sur plus de cores. Si on additionne le temps CPU de tous les cores, on arrive Ã  un total similaire Ã  NumPy.

**Wall time vs CPU time** :
- **Wall time** : OpenCV gagne 4.6Ã— (56.57s â†’ 12.35s) grÃ¢ce au parallÃ©lisme
- **CPU time** : OpenCV gagne seulement 7% (60.09s â†’ 56.00s)

Si la machine fait d'autres choses en parallÃ¨le, monopoliser 4.5 cores (OpenCV) vs 1.1 core (NumPy) peut Ãªtre problÃ©matique. Le gain en wall time se paie par une occupation CPU plus Ã©levÃ©e.

**EfficacitÃ© parallÃ¨le** :
- NumPy : 1.06Ã— speedup (quasi linÃ©aire, pas de surcoÃ»t de parallÃ©lisation)
- OpenCV : 4.53Ã— speedup mais consomme 4.5 cores â†’ efficacitÃ© = 4.53/4.5 = **100%** (excellent)

Le parallÃ©lisme d'OpenCV est bien implÃ©mentÃ© (pas de surcoÃ»t significatif), mais Ã§a ne rÃ©duit pas la charge CPU totale - juste la durÃ©e.

**âœ… Points forts**

- **Wall time excellent** : 4.6Ã— plus rapide que NumPy en temps rÃ©el. IdÃ©al pour pipelines interactifs.
- **Multithreading automatique** : ~4.5 cores utilisÃ©s sans une ligne de code multithread.
- **ParallÃ©lisme efficace** : 100% d'efficacitÃ© (4.53Ã— speedup sur 4.5 cores).
- **Projection simplifiÃ©e** : Formules classiques, plus de constantes magiques Ã  ajuster.
- **Code Python propre** : Pas de compilation C++, juste `pip install opencv-python`.
- **MÃ©moire stable** : 288 MB, similaire Ã  NumPy, bien mieux que FFmpeg (1761 MB).

**âŒ Points faibles**

- **Gain CPU marginal** : Seulement 7% de CPU Ã©conomisÃ© vs NumPy. Si la machine est chargÃ©e, le gain peut Ãªtre contre-productif (on monopolise plus de cores).
- **DÃ©pendance OpenCV** : ~90 MB Ã  installer (vs ~20 MB pour NumPy). Installation parfois capricieuse.
- **Clipping obligatoire** : `cv2.remap()` vÃ©rifie systÃ©matiquement les bornes. Impossible de le dÃ©sactiver mÃªme si on a prÃ©-clippÃ© les coordonnÃ©es.
- **Conversion de types** : Maps en `float32` obligatoire. Conversion depuis `int32` Ã  chaque appel.
- **Moins flexible** : BoÃ®te noire. Impossible de tweaker l'implÃ©mentation de `remap()`.

### Verdict

OpenCV Python apporte un **gain en wall time de 4.6Ã—** grÃ¢ce au multithreading, mais consomme **presque autant de CPU total** que NumPy (7% d'Ã©conomie seulement).

**Cette implÃ©mentation est adaptÃ©e pour** :
- Pipelines interactifs oÃ¹ la latence compte (wall time critique)
- Machines dÃ©diÃ©es oÃ¹ monopoliser 4-5 cores n'est pas un problÃ¨me
- Applications nÃ©cessitant aussi d'autres fonctions OpenCV (Ã©vite une dÃ©pendance supplÃ©mentaire)

**InadaptÃ©e pour** :
- Serveurs partagÃ©s avec forte charge CPU (monopolise trop de cores pour peu de gain)
- Traitement batch oÃ¹ la charge CPU totale prime sur le wall time
- Environnements oÃ¹ OpenCV est difficile Ã  dÃ©ployer

**Question** : Peut-on faire mieux en sortant de Python ? La section suivante explore OpenCV C++ appelÃ© via des bindings Python - mÃªme algorithme mais code natif compilÃ©. Est-ce que Ã§a amÃ©liore le CPU time ou seulement le wall time ?

**Code complet** : [github.com/TON_USER/fisheye-dewarping/tree/main/04_opencv_python](lien-Ã -adapter)

---


## 2.5 OpenCV C++ - Code natif compilÃ©

### L'approche

AprÃ¨s avoir explorÃ© les limites du Python, passons au **C++ natif**. MÃªme algorithme, mÃªme `cv::remap()`, mais cette fois compilÃ© directement en binaire sans interprÃ©teur Python. 

Objectif : mesurer le coÃ»t rÃ©el de l'interprÃ©tation Python et voir si le C++ apporte un gain significatif.

### Le code C++

Le code reprend exactement la mÃªme logique que la version Python OpenCV, mais en C++ natif. L'implÃ©mentation est directe, sans surprises - quaternions, matrices de rotation, projection sphÃ©rique, puis appel Ã  `cv::remap()`.

### DÃ©tails d'implÃ©mentation

**Impact de l'interpolation : diffÃ©rence Python vs C++**

Le choix d'interpolation a un impact **trÃ¨s diffÃ©rent** selon qu'on est en Python ou en C++ :

**OpenCV C++** :
- `INTER_NEAREST` : 7.00s wall, 23.01s CPU
- `INTER_LINEAR` : 12.87s wall, 58.32s CPU
- **Impact : 2.5Ã— plus de CPU, 1.8Ã— plus lent**

**OpenCV Python** :
- `INTER_NEAREST` : 12.35s wall, 56.00s CPU
- `INTER_LINEAR` : 14.46s wall, 72.32s CPU
- **Impact : 1.3Ã— plus de CPU, 1.2Ã— plus lent**

**Explication** : En Python, l'overhead de l'interprÃ©teur et des conversions NumPy/OpenCV "noie" partiellement le coÃ»t de l'interpolation. Le temps passÃ© dans les couches Python est incompressible et masque l'impact du choix d'interpolation.

En C++ pur, **100% du temps est dans le code critique** (`cv::remap()`). Chaque cycle CPU compte. Le surcoÃ»t de l'interpolation linÃ©aire (4 accÃ¨s mÃ©moire + calculs vs 1 accÃ¨s) devient dominant.

**Pour nos benchmarks**, on utilise `INTER_NEAREST` pour maximiser les performances et avoir une comparaison Ã©quitable. Pour de la production, `INTER_LINEAR` reste un choix valide si la qualitÃ© visuelle prime.

### Benchmark

**Configuration : `cv::INTER_NEAREST`** (tous les benchmarks suivants)
```
Commande: ./unwarper ../images/fisheye.jpg --repeat-dewarp 1000

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              7.00s
CPU time (user+sys):     23.01s
  - User time:           21.27s
  - System time:         1.74s
CPU utilization:        328%
Cores utilises:         ~3.3
Memoire pic:            109.95 MB (112584 KB)
======================================================================

Speedup parallele:      3.29x
(CPU time / Wall time = 23.01s / 7.00s)
```

**Comparaisons** (toutes sur 1000 frames Ã— 5 vues, `INTER_NEAREST`) :
- NumPy vectorisÃ© : 56.57s wall, 60.09s CPU, 1.1 cores
- OpenCV Python : 12.35s wall, 56.00s CPU, 4.5 cores
- **OpenCV C++ : 7.00s wall, 23.01s CPU, 3.3 cores**

### Analyse

**Comparaison avec OpenCV Python** :
- **Wall time** : 7.00s vs 12.35s â†’ **1.8Ã— plus rapide**
- **CPU time** : 23.01s vs 56.00s â†’ **2.4Ã— moins de CPU consommÃ©**
- **MÃ©moire** : 110 MB vs 288 MB â†’ **2.6Ã— moins de RAM**
- **Cores** : 3.3 vs 4.5 â†’ lÃ©gÃ¨rement moins parallÃ¨le

**Comparaison avec NumPy** :
- **Wall time** : 7.00s vs 56.57s â†’ **8.1Ã— plus rapide**
- **CPU time** : 23.01s vs 60.09s â†’ **2.6Ã— moins de CPU**

**Le gain du C++ est-il significatif ?**

Par rapport Ã  OpenCV Python, le gain est **modÃ©rÃ© mais rÃ©el** :
- 1.8Ã— en wall time (pas spectaculaire)
- 2.4Ã— en CPU time (meilleur, Ã©conomie significative sur serveur partagÃ©)
- 2.6Ã— en mÃ©moire (le gain le plus notable)

**Pourquoi si peu de diffÃ©rence avec Python ?**

Dans les deux cas (Python et C++), on passe **la majoritÃ© du temps dans `cv::remap()`**, qui est du code C++ optimisÃ©. L'overhead Python ne reprÃ©sente qu'une fraction du temps total :
- Appel de fonction Python â†’ C (via l'API C de Python)
- Gestion des rÃ©fÃ©rences et GC Python
- Conversions de types NumPy â†” cv::Mat

Le ratio 12.35s / 7.00s â‰ˆ 1.8Ã— reprÃ©sente cet overhead Python de ~40% du temps total.

**Le multi-threading diffÃ¨re lÃ©gÃ¨rement** : OpenCV Python (4.5 cores) vs OpenCV C++ (3.3 cores). DiffÃ©rence probablement due aux configurations OpenMP ou au binding Python qui peut forcer plus de parallÃ©lisme.

**Les vrais gains : CPU et mÃ©moire** :
- **CPU time** : 2.4Ã— moins de charge (56s â†’ 23s). Sur un serveur partagÃ© qui traite N flux en parallÃ¨le, cette Ã©conomie est significative.
- **MÃ©moire** : 2.6Ã— moins (288 MB â†’ 110 MB). Les objets Python (wrappers NumPy, compteurs de rÃ©fÃ©rences, dictionnaires d'attributs) ajoutent un overhead de ~180 MB. En C++, les `cv::Mat` sont des structures compactes.

**âœ… Points forts**

- **CPU time rÃ©duit** : 23.01s vs 56.00s (OpenCV Python). Gain de 2.4Ã— sur la charge CPU totale - critique pour serveurs partagÃ©s.
- **MÃ©moire minimale** : 110 MB, 2.6Ã— moins qu'OpenCV Python. Excellent pour environnements contraints ou traitement de nombreux flux simultanÃ©s.
- **Multi-threading efficace** : 3.3 cores, speedup de 3.29Ã— â†’ efficacitÃ© parallÃ¨le de 100%.
- **Pas d'overhead Python** : Pas de GC, pas d'interprÃ©teur, pas de conversions de types. Code natif direct.
- **Performances prÃ©dictibles** : Comportement dÃ©terministe, pas de pauses GC alÃ©atoires qui peuvent causer des latences.
- **SensibilitÃ© claire aux optimisations** : L'impact de l'interpolation est mesurable (2.5Ã—), permettant de choisir prÃ©cisÃ©ment le trade-off performance/qualitÃ©.

**âŒ Points faibles**

- **Compilation nÃ©cessaire** : CMake, toolchain C++ (GCC/Clang), headers OpenCV dev. Beaucoup plus complexe que `pip install opencv-python`.
- **Gain modÃ©rÃ© sur Python** : Seulement 1.8Ã— en wall time. Si OpenCV Python suffit dÃ©jÃ , le C++ n'apporte pas un game changer.
- **Moins flexible** : Toute modification du code = recompilation complÃ¨te. Cycle de dÃ©veloppement plus lent qu'en Python.
- **PortabilitÃ© limitÃ©e** : Binaires spÃ©cifiques Ã  la plateforme (Linux x64, Windows, macOS, ARM). Python est portable out-of-the-box.
- **Debugging plus lourd** : GDB, Valgrind, compilation en mode debug vs simple `print()` en Python.
- **DÃ©pendances de build** : NÃ©cessite OpenCV compilÃ© avec les bonnes options (OpenMP, optimisations). Gestion de dÃ©pendances plus complexe.

### Verdict

OpenCV C++ apporte un **gain modeste mais rÃ©el** : **2.4Ã— moins de CPU, 2.6Ã— moins de RAM, 1.8Ã— plus rapide**.

**Le gain n'est pas spectaculaire** car OpenCV Python passait dÃ©jÃ  la majoritÃ© du temps dans du code C++. L'overhead Python reprÃ©sentait environ 40% du temps total - maintenant Ã©liminÃ©.

**Cette implÃ©mentation est adaptÃ©e pour** :
- **Serveurs partagÃ©s** oÃ¹ Ã©conomiser 2.4Ã— de CPU sur chaque flux compte (traitement de dizaines de flux simultanÃ©s)
- **Applications contraintes en mÃ©moire** (110 MB vs 288 MB permet de traiter plus de flux en parallÃ¨le)
- **Environnements embedded ou edge** oÃ¹ Python est difficile Ã  dÃ©ployer ou trop lourd
- **Besoin de performances dÃ©terministes** (pas de GC qui pause alÃ©atoirement)
- **Optimisation fine** : l'impact clair des choix d'interpolation permet de tuner prÃ©cisÃ©ment

**Pas nÃ©cessaire si** :
- OpenCV Python suffit dÃ©jÃ  (gain de 1.8Ã— en wall time peut ne pas justifier la complexitÃ©)
- La simplicitÃ© de dÃ©ploiement et maintenance prime
- L'agilitÃ© du dÃ©veloppement Python est critique (prototypage rapide, modifications frÃ©quentes)
- Pas de contraintes fortes sur CPU ou mÃ©moire

**Peut-on faire encore mieux ?** La section suivante explore une **bibliothÃ¨que C++ custom** Ã©crite from scratch avec gestion mÃ©moire fine, et algorithmes spÃ©cifiques Ã  notre cas d'usage (camÃ©ras fixes, vues statiques). Peut-on descendre significativement sous 7 secondes ? Ã€ quel prix en complexitÃ© et maintenabilitÃ© ?

**Code complet** : [github.com/TON_USER/fisheye-dewarping/tree/main/05_opencv_cpp](lien-Ã -adapter)

---

## 2.6 BibliothÃ¨que C++ custom optimisÃ©e - L'optimisation ultime

### L'approche

AprÃ¨s avoir explorÃ© OpenCV, passons Ã  une **bibliothÃ¨que C++ Ã©crite from scratch** et optimisÃ©e spÃ©cifiquement pour notre cas d'usage. Plus de dÃ©pendance Ã  OpenCV - juste du C++ pur avec gestion mÃ©moire fine et algorithme minimal.

L'idÃ©e : garder la simplicitÃ© d'appel Python (via ctypes) tout en exploitant des optimisations impossibles avec OpenCV :
- Code minimal sans overhead de bibliothÃ¨que gÃ©nÃ©rique
- Gestion mÃ©moire optimale (buffers rÃ©utilisables)
- Algorithme spÃ©cialisÃ© pour notre cas (camÃ©ras fixes, pas de recalibration)
- Pas de multithreading (Ã©vite la contention, optimal pour mono-flux)

### Architecture

**CÃ´tÃ© C++** : Une bibliothÃ¨que partagÃ©e (.so) exposant une API simple :
```cpp
extern "C" {
    // CrÃ©er le contexte de dewarping (calcule le mapping une fois)
    DewarpContext* create_dewarp_context(int width, int height, int zones);
    
    // Appliquer le dewarping (rapide, appelÃ© en boucle)
    void dewarp_frame(DewarpContext* ctx, uint8_t* input, uint8_t* output, int zone_id);
    
    // LibÃ©rer le contexte
    void free_dewarp_context(DewarpContext* ctx);
}
```

**CÃ´tÃ© Python** : Wrapper ctypes minimal :
```python
import ctypes
import numpy as np

# Charger la bibliothÃ¨que
lib = ctypes.CDLL('libunwarper_ctypes.so')

# Configurer les signatures
lib.create_dewarp_context.argtypes = [c_int, c_int, c_int]
lib.create_dewarp_context.restype = c_void_p

lib.dewarp_frame.argtypes = [c_void_p, POINTER(c_uint8), POINTER(c_uint8), c_int]
lib.dewarp_frame.restype = None

# Utilisation
ctx = lib.create_dewarp_context(1920, 1920, 5)
lib.dewarp_frame(ctx, input_ptr, output_ptr, zone_id)
```

### Optimisations clÃ©s

**1. Table de mapping compacte** : Stockage en `int16_t` au lieu de `float32` (OpenCV). Ã‰conomie mÃ©moire et meilleure localitÃ© cache.

**2. Boucle de remapping optimisÃ©e** :
```cpp
void dewarp_frame(const DewarpContext* ctx, const uint8_t* input_data, 
                  uint8_t* output_data, const int zone_id) {
    const auto* remap_ptr = get_zone_remap_data(ctx, zone_id);
    
    // Traitement ligne par ligne avec buffer local
    for (int j = 0; j < ctx->output_height; ++j) {
        uint8_t buffer[4096];  // Buffer stack, ultra-rapide
        
        for (int i = 0; i < ctx->output_width; ++i) {
            const int remap_offset = (j * ctx->output_width + i);
            const int16_t src_x = remap_ptr[remap_offset * 2];
            const int16_t src_y = remap_ptr[remap_offset * 2 + 1];
            
            const int src_offset = (src_y * ctx->width + src_x) * 3;
            
            // Copie RGB directe dans buffer local
            buffer[i*3]     = input_data[src_offset];
            buffer[i*3 + 1] = input_data[src_offset + 1];
            buffer[i*3 + 2] = input_data[src_offset + 2];
        }
        // Copie groupÃ©e du buffer vers output
        memcpy(output_data + j * ctx->output_width * 3, buffer, ctx->output_width * 3);
    }
}
```

**Pourquoi c'est rapide** :
- **Buffer local sur la stack** : Ã‰vite les allocations dynamiques rÃ©pÃ©tÃ©es
- **AccÃ¨s sÃ©quentiels** : Maximise l'utilisation du cache CPU
- **Pas de vÃ©rification de bornes** : CoordonnÃ©es prÃ©-clippÃ©es dans le mapping
- **Pas de multithreading** : ZÃ©ro overhead de synchronisation ou contention

**3. Pas d'interpolation** : Plus proche voisin uniquement. Suffisant pour dÃ©tection d'objets.

### Benchmark
```
Commande: python3 unwarper_ctypes.py ../images/fisheye.jpg --repeat-dewarp 1024

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              4.91s
CPU time (user+sys):     5.52s
  - User time:           5.48s
  - System time:         0.04s
CPU utilization:        112%
Cores utilises:         ~1.1
Memoire pic:            80.14 MB (82068 KB)
======================================================================

Speedup parallele:      1.12x
(CPU time / Wall time = 5.52s / 4.91s)
```

**Comparaisons** (toutes sur 1024 frames Ã— 5 vues) :
- FFmpeg : 208.64s wall, 1411.84s CPU, 6.8 cores, 1784 MB
- Python pur : 1889.36s wall, 1889.34s CPU, 1.0 core, 647 MB
- NumPy : 110.11s wall, 113.83s CPU, 1.0 core, 256 MB
- OpenCV Python : 22.21s wall, 105.70s CPU, 4.8 cores, 288 MB
- OpenCV C++ : 10.09s wall, 48.90s CPU, 4.8 cores, 110 MB
- **Lib C++ custom : 4.91s wall, 5.52s CPU, 1.1 core, 80 MB**

### Analyse

**Gains spectaculaires sur toutes les mÃ©triques** :

**vs OpenCV C++ :**
- **2.1Ã— plus rapide** en wall time (10.09s â†’ 4.91s)
- **8.9Ã— moins de CPU** (48.90s â†’ 5.52s)
- **1.4Ã— moins de mÃ©moire** (110 MB â†’ 80 MB)

**vs OpenCV Python :**
- **4.5Ã— plus rapide** en wall time
- **19Ã— moins de CPU**
- **3.6Ã— moins de mÃ©moire**

**vs FFmpeg :**
- **42Ã— plus rapide** en wall time
- **256Ã— moins de CPU**
- **22Ã— moins de mÃ©moire**

**D'oÃ¹ viennent ces gains massifs ?**

**1. Pas de multithreading = efficacitÃ© maximale**

Le paradoxe : on utilise **1.1 core** contre 4.8 pour OpenCV C++, mais on est **2.1Ã— plus rapide** en wall time.

Explication : Le multithreading OpenCV a un **coÃ»t cachÃ©** :
- Synchronisation entre threads (mutex, barriers)
- Contention sur le cache (false sharing)
- Context switches frÃ©quents (117k pour OpenCV vs 112 pour nous)
- Overhead de crÃ©ation/destruction de threads

Notre code mono-thread Ã©vite tout Ã§a. Un seul thread qui tourne Ã  fond, accÃ¨s mÃ©moire sÃ©quentiels, cache CPU optimal.

**EfficacitÃ© par core** :
- OpenCV C++ : 10.09s / 4.8 cores = **2.10s/core**
- Lib custom : 4.91s / 1.1 core = **4.46s/core**

Attends, 4.46 > 2.10 ? Non ! C'est un piÃ¨ge de mesure. Le vrai indicateur c'est le **CPU time total** :
- OpenCV C++ : **48.90s de CPU consommÃ©**
- Lib custom : **5.52s de CPU consommÃ©**

On consomme **8.9Ã— moins de ressources CPU** pour le mÃªme travail.

**2. Code minimal sans overhead**

OpenCV `cv::remap()` est une fonction gÃ©nÃ©rique qui gÃ¨re :
- Multiples types d'interpolation
- Multiples types de bordures
- Support GPU optionnel
- VÃ©rifications de validitÃ©
- Abstraction cv::Mat avec compteurs de rÃ©fÃ©rences

Notre code fait **exactement ce dont on a besoin, rien de plus** :
- Interpolation nearest neighbor uniquement
- Bordures prÃ©-gÃ©rÃ©es (clipping dans le mapping)
- Pas d'abstraction, juste des pointeurs bruts
- Pas de vÃ©rifications en phase critique

**3. Gestion mÃ©moire optimale**

- **80 MB** vs 110 MB (OpenCV C++) : Ã‰conomie de 30 MB
- Table de mapping en `int16_t` : 2Ã— plus compact que `float32`
- Pas de structures OpenCV (`cv::Mat` avec headers, refcounting)
- Buffer temporaire sur la stack (pas de malloc)

**4. LocalitÃ© mÃ©moire parfaite**

Le buffer local ligne par ligne maximise l'utilisation du cache L1/L2. Tous les accÃ¨s sont dans ~4KB de donnÃ©es (une ligne), qui tient entiÃ¨rement dans le cache L1 (32KB sur CPU modernes).

**âœ… Points forts**

- **Performances absolues** : Le plus rapide de toutes les implÃ©mentations, sur toutes les mÃ©triques.
- **EfficacitÃ© CPU exceptionnelle** : 5.52s de CPU pour 5120 vues. Imbattable.
- **Empreinte mÃ©moire minimale** : 80 MB seulement. Permet de traiter massivement en parallÃ¨le.
- **SimplicitÃ© d'appel Python** : Wrapper ctypes trivial, pas besoin de compiler des bindings complexes.
- **Pas de dÃ©pendances** : Juste stdlib C++17. Pas de OpenCV, pas de libs tierces.
- **Mono-thread optimal** : Pas de contention, pas de synchronisation. IdÃ©al pour traiter N flux en parallÃ¨le.
- **DÃ©ploiement simple** : Un seul .so Ã  compiler, pas de dÃ©pendances dynamiques.

**âŒ Points faibles**

- **Code C++ Ã  maintenir** : Toute modification nÃ©cessite recompilation.
- **Pas de multithreading** : Si on traite UN SEUL flux trÃ¨s lourd, on n'exploite pas le multi-core. Mais notre use case = N flux en parallÃ¨le.
- **Interpolation fixe** : Nearest neighbor uniquement. Pas de linear/cubic. Acceptable pour dÃ©tection, pas pour qualitÃ© photographique.
- **Compilation nÃ©cessaire** : CMake, toolchain C++. Plus complexe que `pip install`.
- **Code spÃ©cialisÃ©** : OptimisÃ© pour notre cas d'usage prÃ©cis (camÃ©ras fixes, vues statiques). Pas gÃ©nÃ©rique.

### Verdict

La bibliothÃ¨que C++ custom reprÃ©sente **l'optimisation ultime** : **256Ã— moins de CPU que FFmpeg, 19Ã— moins qu'OpenCV Python, 8.9Ã— moins qu'OpenCV C++**.

**Cette implÃ©mentation est parfaite pour** :
- **Production haute performance** : Traiter des dizaines de flux simultanÃ©s avec efficacitÃ© maximale
- **Serveurs partagÃ©s** : Minimise la charge CPU totale (5.52s vs 48.90s pour OpenCV C++)
- **Environnements contraints en mÃ©moire** : 80 MB seulement
- **Applications nÃ©cessitant performances prÃ©dictibles** : Mono-thread, pas de GC, pas de contention

**Trade-offs assumÃ©s** :
- Pas de multithreading (volontairement)
- Pas de flexibilitÃ© (interpolation fixe)
- Code spÃ©cialisÃ© (pas de gÃ©nÃ©ricitÃ© OpenCV)

**Ces trade-offs sont acceptables** parce que notre use case le permet :
- On traite N flux en parallÃ¨le (pas besoin de multi-thread par flux)
- Nearest neighbor suffit pour la dÃ©tection
- CamÃ©ras fixes = pas besoin de recalibration dynamique

**Code complet** : [github.com/pykoder/fisheye-dewarping/tree/main/06_ctypes_custom](lien-Ã -adapter)

---

## Conclusion : Choisir la bonne arme

AprÃ¨s avoir comparÃ© 6 implÃ©mentations diffÃ©rentes du mÃªme algorithme, voici ce qu'on a appris :

### RÃ©capitulatif des performances

**Pour 1024 frames Ã— 5 vues (5120 images dewarpÃ©es) :**

| ImplÃ©mentation | Wall Time | CPU Time | Cores | MÃ©moire | Speedup vs FFmpeg |
|----------------|-----------|----------|-------|---------|-------------------|
| **FFmpeg** | 208.64s | 1411.84s | 6.8 | 1784 MB | 1Ã— (baseline) |
| **Python pur** | 1889.36s | 1889.34s | 1.0 | 647 MB | 0.11Ã— |
| **NumPy vectorisÃ©** | 110.11s | 113.83s | 1.0 | 256 MB | 1.9Ã— |
| **OpenCV Python** | 22.21s | 105.70s | 4.8 | 288 MB | 9.4Ã— |
| **OpenCV C++** | 10.09s | 48.90s | 4.8 | 110 MB | 20.7Ã— |
| **Lib C++ custom** | **4.91s** | **5.52s** | 1.1 | **80 MB** | **42.5Ã—** |

### LeÃ§ons apprises

**1. Le multithreading n'est pas toujours la rÃ©ponse**

FFmpeg (6.8 cores) et OpenCV (4.8 cores) parallÃ©lisent agressivement... mais consomment **Ã©normÃ©ment de CPU total**. La lib custom mono-thread (1.1 core) est **256Ã— plus efficiente en CPU**.

**MoralitÃ©** : Sur un serveur qui traite N flux en parallÃ¨le, mieux vaut N processus mono-thread efficients qu'un processus multi-thread qui monopolise tous les cores.

**2. L'overhead Python est rÃ©el mais pas dramatique**

OpenCV Python vs OpenCV C++ : facteur 2.4Ã— en CPU, 2.6Ã— en mÃ©moire. Acceptable pour beaucoup d'use cases. Si Python suffit, pas besoin de passer au C++.

**3. La vectorisation NumPy a ses limites**

NumPy vectorisÃ© = 17Ã— plus rapide que Python pur, mais reste 4.5Ã— plus lent qu'OpenCV Python. L'indexation avancÃ©e NumPy ne parallÃ©lise pas bien.

**4. Le code spÃ©cialisÃ© Ã©crase le code gÃ©nÃ©rique**

Lib custom vs OpenCV C++ : 8.9Ã— moins de CPU. Pourquoi ? Parce qu'on fait **exactement ce dont on a besoin**, sans l'overhead d'une bibliothÃ¨que gÃ©nÃ©rique.

**5. La mÃ©moire compte**

80 MB (custom) vs 1784 MB (FFmpeg) = **22Ã— moins**. Sur un serveur traitant 20 flux simultanÃ©s :
- Custom : 20 Ã— 80 MB = 1.6 GB
- FFmpeg : 20 Ã— 1784 MB = 35 GB (impossible)

### Recommandations par use case

**Prototypage rapide / POC**
â†’ **FFmpeg CLI**
- Setup immÃ©diat, aucun code
- Performances correctes
- Limitation : pas intÃ©grable, grosse consommation RAM

**Pipeline Python existant, performances OK**
â†’ **OpenCV Python**
- IntÃ©gration triviale (`pip install`)
- Performances acceptables (9.4Ã— vs FFmpeg en wall time)
- Limitation : consomme beaucoup de CPU (105.70s)

**Pipeline Python, besoin d'optimiser**
â†’ **Lib C++ custom via ctypes**
- Performances maximales tout en restant appelable depuis Python
- EfficacitÃ© CPU exceptionnelle
- Limitation : nÃ©cessite compilation, maintenance C++

**Application standalone, performance critique**
â†’ **OpenCV C++ ou lib custom**
- OpenCV C++ si besoin de flexibilitÃ© (interpolation, etc.)
- Lib custom si performance absolue requise
- Limitation : complexitÃ© de build/dÃ©ploiement

**Apprentissage / comprÃ©hension**
â†’ **Python pur**
- Code pÃ©dagogique
- Chaque Ã©tape mathÃ©matique explicite
- Limitation : 20Ã— trop lent pour production

### Le retour d'expÃ©rience

**Il y a trois ans**, face au memory leak de la lib propriÃ©taire, on a codÃ© une solution C++ en 15 jours. Ã‡a a marchÃ©.

**Aujourd'hui**, avec le recul, qu'aurions-nous fait diffÃ©remment ?

Probablement... **exactement pareil**. La lib C++ custom optimisÃ©e Ã©tait le bon choix :
- Performances exceptionnelles (nÃ©cessaire pour traiter N flux)
- Consommation mÃ©moire minimale (critique en prod)
- Pas de dÃ©pendances externes (pas de risque de nouveau memory leak)
- Code maÃ®trisÃ© de bout en bout (pas de boÃ®te noire)

FFmpeg aurait Ã©tÃ© une solution de secours acceptable en attendant mieux, mais on aurait vite Ã©tÃ© limitÃ©s (RAM, intÃ©gration, contrÃ´le).

### Et maintenant ?

Ce benchmark comparatif nous confirme qu'on a fait le bon choix technologique il y a trois ans. Mais il rÃ©vÃ¨le aussi des pistes d'amÃ©lioration :

**Optimisations possibles sur la lib custom :**
- Instructions SIMD explicites (AVX2) pour le remapping
- Prefetching mÃ©moire plus agressif
- Support GPU via CUDA (pour les trÃ¨s gros volumes)

**Mais** : Vu les performances actuelles (4.91s pour 5120 vues), est-ce vraiment nÃ©cessaire ? Le jus vaut-il la chandelle ?

**La vraie question** : Ã€ quel moment l'optimisation devient-elle de la sur-ingÃ©nierie ?

Pour notre use case (traitement temps rÃ©el de multiples flux), **la lib custom actuelle est largement suffisante**. Les 4.91s de wall time et 5.52s de CPU reprÃ©sentent moins de 1ms par vue - amplement suffisant pour du temps rÃ©el.

**Conclusion** : Avant d'optimiser, mesurez. Avant de mesurer, dÃ©finissez vos contraintes. Et surtout : **la solution la plus simple qui marche est souvent la meilleure**.

---

## Remerciements

Merci Ã  Damien pour les maths derriÃ¨re l'algorithme (quaternions et projections sphÃ©riques), et Ã  toute l'Ã©quipe de Veesion pour m'avoir supportÃ© pendant cinq ans.

Le code complet des 6 implÃ©mentations est disponible sur GitHub : [github.com/pykoder/fisheye-dewarping](lien-Ã -adapter)

---

*Article Ã©crit en dÃ©cembre 2025. Les benchmarks ont Ã©tÃ© rÃ©alisÃ©s sur un Lenovo ThinkPad P14s - Ubuntu 25.04, Intel Core i7-1185G7 (4 cores physiques, 8 threads), 16GB RAM. Le CPU supportant AVX-512, les performances NumPy/OpenCV bÃ©nÃ©ficient des instructions SIMD avancÃ©es.*