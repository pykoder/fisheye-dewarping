# Fisheye Dewarping : Du Memory Leak Ã  6 ImplÃ©mentations ComparÃ©es

*TL;DR : Face Ã  un bug critique en prod, j'ai dÃ» recoder un dewarper fisheye from scratch. Trois ans plus tard, je revisite ce problÃ¨me pour comparer 6 approches diffÃ©rentes - FFmpeg ligne de commande, Python pur, NumPy vectorisÃ©, OpenCV Python/C++, et une lib C++ custom optimisÃ©e.*

---

## L'incident Fisheye

**Janvier 2022.** Notre conteneur Docker plante en prod aprÃ¨s quelques heures sur certains sites. Le diagnostic tombe : memory leak dans la bibliothÃ¨que propriÃ©taire qui gÃ¨re le dewarping des camÃ©ras fisheye 360Â°.

Contexte rapide : notre IA analyse des flux vidÃ©o pour dÃ©tecter des gestes. Nos modÃ¨les sont entraÃ®nÃ©s sur des vues plates, pas sur des vues circulaires dÃ©formÃ©es. Sans dewarping, **zÃ©ro dÃ©tection** sur les camÃ©ras fisheye, juste 1% du parc, mais des clients importants.

La lib qui plante ? Fournie par le fabricant des camÃ©ras. On ne peut ni la patcher, ni attendre un correctif.

**Verdict** : on code notre propre version.

Quinze jours de plongÃ©e dans les bouquins de gÃ©omÃ©trie projective, une implÃ©mentation C++ Ã  base de quaternions et projections sphÃ©riques, quelques semaines d'optimisation... et on avait notre solution en prod.

**Trois ans plus tard**, avec un peu de recul et du temps libre, une question me trotte dans la tÃªte : **et si on l'avait fait autrement ?**

Quelle aurait Ã©tÃ© la meilleure approche avec nos contraintes ?
- CamÃ©ras fisheye **fixes** au plafond
- Besoin de **5 vues plates** par frame
- Points de vue des camÃ©ras virtuelles **statiques** (pas de rotation dynamique)
- **Performance critique** : traitement temps rÃ©el de multiples flux vidÃ©o
- QualitÃ© "suffisante" pour la dÃ©tection (pas besoin de perfection photographique)

Cet article compare **6 implÃ©mentations diffÃ©rentes** pour ce cas d'usage prÃ©cis, avec benchmarks Ã  l'appui.

---

## Ce que vous allez dÃ©couvrir

1. **Les bases thÃ©oriques** du dewarping fisheye (version courte, promis)
2. **Trois implÃ©mentations** du mÃªme algorithme :
   - FFmpeg en ligne de commande
   - Python natif (boucles, pas de lib)
   - NumPy vectorisÃ©
3. **Benchmarks comparatifs** : temps d'exÃ©cution, RAM, complexitÃ© de mise en Å“uvre
4. Une mise en bouche pour la **partie 2** au cours de laquelle nous explorons OpenCV et une implÃ©mentation C++ ad hoc.

**Important** : Cette comparaison est spÃ©cifique Ã  *notre* cas d'usage (camÃ©ras fixes, vues statiques, perf temps rÃ©el). Si vos besoins diffÃ©rent - camÃ©ras mobiles, recalibration dynamique, qualitÃ© maximale, etc. - adaptez en consÃ©quence.

---

## Un peu de thÃ©orie (juste ce qu'il faut)

### Le problÃ¨me en image

Une camÃ©ra fisheye 360Â° au plafond capture tout l'espace environant dans une image circulaire dÃ©formÃ©e.

Pour que nos algorithmes de dÃ©tection puissent travailler, nous en tirons plusieurs vues plates rectangulaires :

![SchÃ©ma de principe](https://github.com/pykoder/fisheye-dewarping/blob/main/images/schema.png?raw=true)


### Comment Ã§a marche (version simplifiÃ©e)

Le dewarping se dÃ©compose en deux phases :

**Phase 1 : Calcul du mapping (une seule fois au dÃ©marrage)**

On crÃ©e une table de correspondance : pour chaque pixel de nos 5 vues plates de sortie, on calcule quel pixel de l'image fisheye il faut aller chercher.

ConcrÃ¨tement :
1. On projette chaque point de l'image fisheye sur une demi-sphÃ¨re virtuelle centrÃ©e sur la camÃ©ra
2. On dÃ©finit 5 "camÃ©ras virtuelles" avec leurs positions et orientations fixes
3. Pour chaque camÃ©ra virtuelle, on calcule quelle portion de la sphÃ¨re elle "voit"
4. On stocke tout Ã§a dans une lookup table

Cette phase utilise des projections sphÃ©riques et dans notre cas initial, des quaternions pour les rotations. C'est du calcul assez lourd, mais on ne le fait **qu'une seule fois**.

**Phase 2 : Application du mapping (pour chaque frame vidÃ©o)**

Pour chaque nouvelle image de la camÃ©ra fisheye :
1. On parcourt nos 5 vues de sortie
2. Pour chaque pixel, on regarde dans la lookup table d'oÃ¹ il vient
3. On copie la couleur du pixel source (avec interpolation optionnelle si on veut de la qualitÃ©)

C'est cette phase qu'on doit optimiser Ã  fond. Elle tourne en boucle sur chaque frame.

### Note sur le calibrage

Les lentilles fisheye varient d'un modÃ¨le de camÃ©ra Ã  l'autre. Un calibrage prÃ©cis permet d'obtenir des vues parfaitement rectilignes. Dans notre cas, nous nous en passons : nos algos de dÃ©tection tolÃ¨rent de lÃ©gÃ¨res dÃ©formations rÃ©siduelles. Cela simplifie le code et booste les performances.

---

## ImplÃ©mentation 1: FFmpeg CLI - La solution rapide et parallÃ¨le

### L'approche

FFmpeg supporte nativement ce type de dewarping via son filtre `v360`. L'implÃ©mentation est directe mais rÃ©vÃ¨le quelques subtilitÃ©s intÃ©ressantes.

### Le code
```bash
#!/bin/bash
# unwarper_ffmpeg.sh

ffmpeg -y -i "fisheye.mp4" \
-vf "crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_1.mp4" \
-vf "rotate=4*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_2.mp4" \
-vf "rotate=3*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_3.mp4" \
-vf "rotate=2*72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_4.mp4" \
-vf "rotate=72*PI/180,crop=1920:1920,v360=input=fisheye:output=flat:interp=near:yaw=0:pitch=45:roll=0:v_fov=90:w=960:h=960" "unwarped_5.mp4"
```

### DÃ©tails d'implÃ©mentation

**ParamÃ¨tres du filtre v360 :**
- `yaw`, `pitch`, `roll` : rotations dÃ©crivant l'orientation de la camÃ©ra fisheye (dome au plafond)
- `v_fov=90` : champ de vision vertical de la vue de sortie
- `w=960:h=960` : rÃ©solution des vues dewarpÃ©es
- `interp=near` : interpolation plus proche voisin (vs `linear` par dÃ©faut)

**Choix d'optimisation :**

1. **Interpolation minimale** : On force `interp=near` au lieu de l'interpolation linÃ©aire par dÃ©faut. La qualitÃ© d'image est lÃ©gÃ¨rement dÃ©gradÃ©e mais les performances sont meilleures. Pour de la dÃ©tection d'objets, c'est largement suffisant.

2. **Rotation de l'image source** : Le contrÃ´le du point de vue via `yaw/pitch/roll` est limitÃ© et ne permet d'obtenir le rÃ©sultat souhaitÃ© que dans une seule direction. Nous contournons le problÃ¨me en appliquant une rotation prÃ©alable de l'image fisheye (multiples de 72Â° pour couvrir les 360Â°).

3. **Lecture unique du fichier source** : Toutes les vues sont gÃ©nÃ©rÃ©es en un seul passage de FFmpeg (une commande avec 5 outputs). Essentiel pour les perfs.

4. **VidÃ©o de 1024 frames** : Les benchmarks utilisent une vidÃ©o suffisamment longue pour que le temps d'application du mapping domine largement le temps de calcul initial du mapping (la phase qui nous intÃ©resse vraiment). 1024 rÃ©pÃ©titions du dewarping sera la rÃ©fÃ©rence pour les autres solution.

### Benchmark
```
Commande: ./unwarper_ffmpeg.sh fisheye_video.mp4

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              208.64s
CPU time (user+sys):     1411.84s
  - User time:           1401.84s
  - System time:         10.00s
CPU utilization:        676%
Cores used:             ~6.8
Peak memory:            1784.20 MB (1827016 KB)
Page faults:            387273 minor, 0 major
Context switches:       328327 vol, 712451 invol
Exit status:            0

Parallel speedup:       6.77x
(CPU time / Wall time = 1411.84s / 208.64s)
======================================================================
```

**RÃ©sultats :** 208 secondes pour traiter 1024 frames et gÃ©nÃ©rer 5 vues, soit 5120 images plates extraites, soit environ 40ms par image plate. 

FFmpeg exploite bien les 8 cores disponibles (~676% d'utilisation CPU), avec un speedup parallÃ¨le de 6.8x. L'utilisation mÃ©moire grimpe toutefois Ã  **1.7 GB**, ce qui est significatif. Cette consommation mÃ©moire ne dÃ©pend pas de la longueur du film.

FFmpeg est aussi un peu pÃ©nalisÃ© vis Ã  vis des autres solutions car il procÃ¨de aussi Ã  un rÃ©encodage des vues en mp4 sous forme de film. Ce rÃ©encodage n'est en rÃ©alitÃ© pas nÃ©cessaire dans le cas d'usage prÃ©sentÃ©.


### Analyse

**âœ… Points forts**
- **Setup immÃ©diat** : une seule commande, aucune lib Ã  installer (FFmpeg suffit)
- **ParallÃ©lisation native** : utilisation optimale du multi-core sans effort
- **ParallÃ¨lization excellente** : accÃ©lÃ©ration de 6,77Ã—
- **Robuste** : FFmpeg est battle-tested en prod partout
- **Pas de maintenance** : dÃ©pendance externe stable, bugs dÃ©jÃ  corrigÃ©s par la communautÃ©

**âŒ Points faibles**
- **BoÃ®te noire totale** : impossible d'auditer ou modifier l'algo de dewarping
- **FlexibilitÃ© limitÃ©e** : on est coincÃ© avec les paramÃ¨tres exposÃ©s par `v360`
- **Pas intÃ©grable finement** : nÃ©cessite de spawner un process externe, impossible d'appeler directement comme une fonction Python
- **Consommation mÃ©moire Ã©levÃ©e** : 1.7 GB pour une vidÃ©o 1920Ã—1920, potentiellement problÃ©matique Ã  grande Ã©chelle
- **Optimisations limitÃ©es** : on ne peut pas optimiser la phase de mapping spÃ©cifiquement pour notre cas d'usage (camÃ©ras fixes, vues statiques)

### Verdict

FFmpeg est **l'arme idÃ©ale pour un POC rapide** ou quand vous avez besoin d'un rÃ©sultat qui marche immÃ©diatement sans vous poser de questions. Parfait pour :
- Tester si le dewarping rÃ©sout le problÃ¨me mÃ©tier
- Scripts one-shot ou batch processing occasionnel
- Situations oÃ¹ la RAM n'est pas une contrainte

**Mais inadaptÃ© si :**
- Vous devez intÃ©grer le dewarping dans un pipeline Python complexe
- Vous voulez optimiser finement (prÃ©-calcul du mapping une fois, rÃ©utilisation)
- La consommation mÃ©moire est critique
- Vous avez besoin de comprendre ou adapter l'algorithme sous-jacent

Dans notre cas (memory leak de la lib propriÃ©taire), FFmpeg aurait pu Ãªtre une solution de secours acceptable... mais nous aurions vite Ã©tÃ© limitÃ©s pour l'optimisation et l'intÃ©gration.


---
## ## Implementation 2: Python pur - Objectif Comprendre les maths

### L'approche

Maintenant qu'on a vu la solution "boÃ®te noire" avec FFmpeg, plongeons dans les entrailles de l'algorithme. Cette implÃ©mentation en **Python pur** utilise uniquement les bibliothÃ¨ques standard et NumPy pour la manipulation de tableaux, mais **sans aucune vectorisation** (ok, j'avoue j'ai laissÃ© un produit matriciel pour ne pas le recoder Ã  la main).

L'objectif ici n'est pas la performance, mais la **comprÃ©hension**. Chaque Ã©tape mathÃ©matique est explicite, documentÃ©e, comprÃ©hensible. C'est la rÃ©fÃ©rence pÃ©dagogique qui servira de baseline pour toutes les optimisations ultÃ©rieures.

### Le code
```python
#!/usr/bin/env python3
"""
Pure Python Fisheye Unwarper

Cette implÃ©mentation en Python pur du dewarping fisheye utilise uniquement
des bibliothÃ¨ques standard et numpy pour manipuler des tableaux,
sans aucune optimisation vectorielle.
"""

def multiply_quaternion(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    Multiply two quaternions using Python primitives.
    
    Args:
        a: First quaternion [w, x, y, z]
        b: Second quaternion [w, x, y, z]
        
    Returns:
        Result quaternion [w, x, y, z]
    """
    w1, x1, y1, z1 = a
    w2, x2, y2, z2 = b
    
    w = w1*w2 - x1*x2 - y1*y2 - z1*z2
    x = w1*x2 + x1*w2 + y1*z2 - z1*y2
    y = w1*y2 - x1*z2 + y1*w2 + z1*x2
    z = w1*z2 + x1*y2 - y1*x2 + z1*w2
    
    return np.array([w, x, y, z], dtype=np.float64)

def get_rotation_matrix(yaw: float, pitch: float, roll: float) -> np.ndarray:
    """
    Generate rotation matrix from yaw, pitch, roll angles (in degrees).
    
    Args:
        yaw: Rotation around Y axis in degrees
        pitch: Rotation around X axis in degrees
        roll: Rotation around Z axis in degrees
    Returns:
        3x3 rotation matrix as numpy array
    """
    # Yaw quaternion, rotate view around Y axis
    yaw = np.deg2rad(0)
    yaw_q = np.array([np.cos(yaw/2.0), 0.0, np.sin(yaw/2.0), 0.0], dtype=np.float64)
    # Pitch quaternion, rotate view around X axis (look up 45 degrees)
    pitch = np.deg2rad(45)             
    pitch_q = np.array([np.cos(pitch/2.0), np.sin(pitch/2.0), 0.0, 0.0], dtype=np.float64)
    # Roll quaternion, rotate view around Z axis (look in different direction)
    roll = np.deg2rad(roll)
    roll_q = np.array([np.cos(roll/2.0), 0.0, 0.0, np.sin(roll/2.0)], dtype=np.float64)

    rq = multiply_quaternion(roll_q, multiply_quaternion(pitch_q, yaw_q))

    # Build spherical projection matrix from quaternions
    w, x, y, z = rq
    return np.array([
        [ (w*w + x*x - y*y - z*z),  2.0 * (x*y - z*w), 2.0 * (w*y + x*z)],
        [ 2.0 * (w*z + x*y), (w*w - x*x + y*y - z*z), 2.0 * (y*z - w*x)],
        [ 2.0 * (x*z - y*w), 2.0 * (w*x + y*z), (w*w - x*x - y*y + z*z)]], dtype=np.float64)


def project2D(xyz: np.ndarray) -> Tuple[int, int]:
    """Project 3D Dome point to 2D Fisheye image"""
    hs = np.hypot(xyz[0],xyz[1])
    phi = np.arctan2(hs, xyz[2])
    coeff = phi / (hs * np.pi)
    src_x = xyz[0] * coeff + 0.5
    src_y = xyz[1] * coeff + 0.5
    return src_x, src_y


class PythonDewarper:
    """
    Pure Python implementation of the fisheye dewarper.    
    """
    
    def __init__(self, width: int, height: int, zones: int = 3):
        """
        Initialize dewarper with image dimensions.
        
        Args:
            width: Image width in pixels
            height: Image height in pixels
        """
        self.width = width
        self.height = height
        self.output_width = self.width // 2
        self.output_height = self.height // 2
        self.zones = zones
        
        # Remapping tables for each view
        self.remap = self._dewarp_mapping()
        self.output_buffer = np.zeros((self.zones, self.output_height, self.output_width, 3), dtype=np.uint8)
    

    def _dewarp_mapping(self) -> List[List[List[Tuple[int, int]]]]:
        """
        Create pixel remapping table for specific view.
        
        This is the core dewarping algorithm using spherical projection.
        """
       
        remap = []
        for zone_id in range(self.zones):
    
            # Get rotation matrix for this zone
            R = get_rotation_matrix(0, 45, zone_id * (360.0 / self.zones))

            remap_zone = []
            for j in range(self.output_height):
                line = []
                for i in range(self.output_width):
                    v = np.array([i / (0.25 * self.width) - 1.0, j / (0.25 * self.height) - 1.0, 1.0])
                    xyz = R @ v.T
                    src_x, src_y = project2D(xyz)
                    map_y = int(src_y * self.height)
                    map_x = int(src_x * self.width)
                    if 0 <= map_y < self.height and 0 <= map_x < self.width:    
                        line.append((map_y, map_x))
                    else:
                        line.append((0, 0))
                remap_zone.append(line)
            remap.append(remap_zone)

        return remap
    
    def dewarp_frame(self, image: np.ndarray, zone_id: int = -1):
        """
        Apply dewarping transformation to image.
        
        Args:
            image: Input image as NumPy array (H, W, 3)
        """
        
        remap_table = self.remap[zone_id]
        output_buffer = self.output_buffer[zone_id]

        for i in range(self.output_height):
            for j in range(self.output_width):
                # Note: never out of bound as it is ensured when building remapping
                output_buffer[i, j] = image[remap_table[i][j]]

        return output_buffer.reshape((self.output_height, self.output_width, 3))
```

### Explication dÃ©taillÃ©e des maths

Maintenant qu'on a le code, dÃ©cortiquons ce qui se passe sous le capot. C'est ici qu'on va plonger dans les dÃ©tails mathÃ©matiques - on n'y reviendra plus dans les sections suivantes.

#### Pourquoi les quaternions ?

**Petit rappel historique** : Les quaternions ont Ã©tÃ© inventÃ©s en 1843 par William Hamilton pour reprÃ©senter les rotations dans l'espace 3D. Ils ont depuis conquis la 3D, la robotique, l'aÃ©rospatiale et... le dewarping fisheye.

**Pourquoi pas des angles d'Euler classiques ?**

Les angles d'Euler (yaw/pitch/roll) sont intuitifs mais ont plusieurs dÃ©fauts :
- **Gimbal lock** : certaines combinaisons de rotations causent une perte de degrÃ© de libertÃ©
- **Interpolation non-linÃ©aire** : difficile d'interpoler proprement entre deux orientations
- **Calculs plus lourds** : composer des rotations nÃ©cessite 3 multiplications matricielles 3Ã—3

Les quaternions rÃ¨glent tout Ã§a :
- **Pas de gimbal lock** : toutes les orientations sont reprÃ©sentables sans singularitÃ©
- **Composition efficace** : multiplier deux quaternions = 16 multiplications + 12 additions (vs 27 mult + 18 add pour des matrices 3Ã—3)
- **Compacts** : 4 nombres au lieu de 9 (matrice 3Ã—3)
- **Normalisables facilement** : Ã©vite l'accumulation d'erreurs numÃ©riques lors de compositions successives

**Structure d'un quaternion** : `[w, x, y, z]` oÃ¹ :
- `w` : partie scalaire (cosinus de la demi-rotation)
- `(x, y, z)` : partie vectorielle (direction de l'axe de rotation)

Une rotation de Î¸ autour d'un axe unitaire `(ax, ay, az)` s'Ã©crit :
```
q = [cos(Î¸/2), axÂ·sin(Î¸/2), ayÂ·sin(Î¸/2), azÂ·sin(Î¸/2)]
```

#### Les trois rotations

Notre camÃ©ra virtuelle doit pouvoir regarder dans n'importe quelle direction. On dÃ©compose Ã§a en 3 rotations.

**Convention importante** : Pour une camÃ©ra fisheye montÃ©e **au plafond** (regardant vers le bas), les axes sont diffÃ©rents de la convention classique des camÃ©ras horizontales :

**1. Yaw (rotation autour de Y)** : rotation de la camÃ©ra sur son axe optique
```python
yaw_q = [cos(yaw/2), 0, sin(yaw/2), 0]
```
Dans notre cas : `yaw=0Â°` (pas de rotation sur l'axe)

**2. Pitch (rotation autour de X)** : inclinaison du regard (regarder plus ou moins vers le bas)
```python
pitch_q = [cos(pitch/2), sin(pitch/2), 0, 0]
```
Ici : `pitch=45Â°` (on regarde vers le bas Ã  45Â° depuis l'horizontale, idÃ©al pour voir le sol + une portion du plafond)

**3. Roll (rotation autour de Z)** : tourner la tÃªte Ã  gauche/droite, balayer l'horizon
```python
roll_q = [cos(roll/2), 0, 0, sin(roll/2)]
```
C'est cette rotation qu'on fait varier : `roll = 360Â° Ã— zone_id / 5` pour couvrir les 360Â° en 5 vues (0Â°, 72Â°, 144Â°, 216Â°, 288Â°)

**Composition** : On combine les trois rotations en multipliant les quaternions :
```python
rq = roll_q Ã— pitch_q Ã— yaw_q
```

L'ordre compte ! Ici on applique d'abord le yaw, puis le pitch, puis le roll. L'ordre de multiplication des quaternions suit la rÃ¨gle : la rotation la plus Ã  droite est appliquÃ©e en premier.

#### De quaternion Ã  matrice de projection

Le quaternion nous donne l'orientation de la camÃ©ra, mais pour projeter les pixels on a besoin d'une **matrice 3Ã—3 de projection sphÃ©rique**.

Cette matrice transforme un point `(x, y)` de l'image de sortie en un point `(X, Y, Z)` sur la demi-sphÃ¨re virtuelle centrÃ©e sur la camÃ©ra fisheye.

Les formules proviennent de la conversion quaternion â†’ matrice de rotation, adaptÃ©e pour la projection sphÃ©rique fisheye. Chaque coefficient de la matrice `m[i,j]` encode comment les coordonnÃ©es `(x, y)` de l'output contribuent aux coordonnÃ©es `(X, Y, Z)` du point 3D.

Les 9 coefficients de la matrice sont calculÃ©s selon ce mÃªme principe, chacun combinant les composantes du quaternion de maniÃ¨re spÃ©cifique pour encoder la rotation 3D complÃ¨te.

#### Projection sphÃ©rique finale

Une fois qu'on a le point 3D `(X, Y, Z)` sur la demi-sphÃ¨re, on doit le re-projeter dans l'image fisheye source :
```python
hs = sqrt(XÂ² + YÂ²)           # Distance horizontale du point
phi = atan2(hs, Z)            # Angle depuis le zÃ©nith (0 Ã  Ï€/2)

src_x = width Ã— (X Ã— phi / (Ï€ Ã— hs) + 0.5)
src_y = height Ã— (Y Ã— phi / (Ï€ Ã— hs) + 0.5)
```

Cette formule implÃ©mente la **projection Ã©quidistante** (equidistant projection), le modÃ¨le standard pour les objectifs fisheye :
- L'angle `phi` (angle depuis le zÃ©nith) est proportionnel Ã  la distance radiale dans l'image fisheye
- `X / hs` et `Y / hs` donnent la direction azimutale normalisÃ©e
- Le facteur `phi / (Ï€ Ã— hs)` convertit l'angle en distance radiale normalisÃ©e [0, 0.5]
- Le `+0.5` centre l'image (passage de [-0.5, 0.5] Ã  [0, 1])

Ce modÃ¨le Ã©quidistant signifie qu'un objet Ã  45Â° du centre apparaÃ®t Ã  mi-chemin entre le centre et le bord de l'image fisheye, un objet Ã  90Â° est exactement au bord.

#### RÃ©sumÃ© du pipeline complet

Pour chaque pixel `(i, j)` de l'image dewarpÃ©e de sortie :

1. **Appliquer l'offset** : `x = i - offset_width`, `y = j - offset_height`
2. **Projection par matrice** : `(X, Y, Z) = m Ã— (x, y, 1)` â†’ point 3D sur la demi-sphÃ¨re
3. **Calcul angle sphÃ©rique** : `phi = atan2(sqrt(XÂ²+YÂ²), Z)` â†’ angle depuis le zÃ©nith
4. **Projection fisheye inverse** : `(src_x, src_y)` dans l'image source fisheye
5. **Copie du pixel** : `output[i,j] = input[src_y, src_x]` (nearest neighbor)

**Phases distinctes** :
- Ã‰tapes 1-4 = **Phase 1 - Calcul du mapping** (exÃ©cutÃ©e une seule fois Ã  l'initialisation)
- Ã‰tape 5 = **Phase 2 - Application du mapping** (exÃ©cutÃ©e pour chaque frame vidÃ©o)

Cette sÃ©paration est cruciale pour les performances : le calcul du mapping est coÃ»teux mais ne se fait qu'une fois. L'application du mapping est rÃ©pÃ©tÃ©e des milliers de fois et doit Ãªtre ultra-optimisÃ©e - c'est lÃ  que les optimisations vont se concentrer dans les sections suivantes.

### Benchmark
```
ðŸ” Commande: uv run ./unwarper_python.py ../images/fisheye.jpg -r 1024

======================================================================
ðŸ“ˆ RÃ‰SULTATS BENCHMARK
======================================================================
â±ï¸  Wall time:              1889.36s
âš™ï¸  CPU time (user+sys):     1889.34s
    â”œâ”€ User time:           1888.94s
    â””â”€ System time:         0.40s
ðŸ”¥ CPU utilization:        99%
ðŸ’» Cores utilisÃ©s:         ~1.0
ðŸ§  MÃ©moire pic:            646 MB (662148 KB)
======================================================================

ðŸ’¡ Speedup parallÃ¨le:      1.00x
   (CPU time / Wall time = 1889.36s / 1889.34s)

```

**RÃ©sultats** : 1889 secondes (31 minutes 29 secondes) pour traiter 1024 frames Ã— 5 vues. Soit environ **369 ms par frame et par vue**.

### Analyse

**Performance comparÃ©e Ã  FFmpeg** :
- FFmpeg : 208.64s pour 1024 frames Ã— 5 vues â†’ **49.7 ms/frame/vue**
- Python pur : 211s pour 1024 frames Ã— 5 vues â†’ **369 ms/frame/vue**
- **Ratio : Python pur est 9.1Ã— plus lent que FFmpeg**

Mais FFMpeg utilise 6.8 cores, tandis que python n'utilise qu'1 core. En termes de **consommation CPU globale**:
- FFmpeg: 1411.84s CPU time â†’ **276 ms CPU time/view**
- Pure Python: 1889.34s CPU time â†’ **369 ms CPU time/view**
- **Ratio: Pure Python uses only 1.3Ã— more CPU than FFmpeg!**

C'est Ã©tonnament efficace pour du Python interprÃ©tÃ©! Bien entendu cela ne prend pas en compte le dÃ©codage et rÃ©encodage vidÃ©o cÃ´tÃ© FFMpeg ce qui explique en partie les rÃ©sultats.


**âœ… Points forts**

- **Code lisible et comprÃ©hensible** : 200 lignes de Python clair oÃ¹ chaque Ã©tape mathÃ©matique est explicite. IdÃ©al pour comprendre l'algorithme, le debugger, ou l'adapter Ã  un nouveau cas d'usage.
- **Consommation mÃ©moire modÃ©rÃ©e** : 650 MB vs 1800 MB pour FFmpeg, soit **2.8Ã— moins**. La table de remapping prÃ©-calculÃ©e est compacte (~9 MB pour 5 vues), et on ne charge qu'une frame Ã  la fois.
- **Baseline de rÃ©fÃ©rence solide** : ImplÃ©mentation correcte et vÃ©rifiÃ©e qu'on peut utiliser comme point de comparaison pour toutes les optimisations futures.
- **Facilement modifiable** : Besoin de changer l'angle de vue ? Les paramÃ¨tres de calibration ? Tout est accessible et modifiable sans recompiler quoi que ce soit.
- **Mono-core total** : CPU Ã  100% signifie qu'on utilise **un seul core**. Le GIL (Global Interpreter Lock) de Python empÃªche le parallÃ©lisme. FFmpeg utilisait 7.7 cores en parallÃ¨le, on reste Ã  1. Dans notre cas d'usage on peut considÃ©rer cela comme une qualitÃ©, car les ressources CPU sont utilisÃ©es efficacement. Python ne consomme que 3 fois plus de ressources CPU que ffmpeg pour le mÃªme traitement.


**âŒ Points faibles**

- **Dramatiquement lent** : 6.7Ã— plus lent que FFmpeg, pas utilisable en production pour du temps rÃ©el.
- **Boucles Python catastrophiques** : On a 960 Ã— 960 Ã— 5 = 4,6 millions d'itÃ©rations de boucles Python par frame. Chaque itÃ©ration implique du bytecode Python interprÃ©tÃ© (accÃ¨s dictionnaire, indexation NumPy, assignation, gestion d'exceptions), ce qui est des ordres de grandeur plus lent que du code natif.
- **Aucune vectorisation** : NumPy est utilisÃ© uniquement comme conteneur de donnÃ©es. On n'exploite **aucune** des optimisations SIMD ou des opÃ©rations vectorielles batch qu'il offre.

### Verdict

La version Python pur est un outil **pÃ©dagogique**, pas une solution de production.

**Ce code est parfait pour** :
- Comprendre exactement comment fonctionne le dewarping fisheye
- Servir de rÃ©fÃ©rence pour vÃ©rifier la correction des implÃ©mentations optimisÃ©es
- Prototyper rapidement des variations de l'algorithme (nouveaux angles, calibrations diffÃ©rentes)
- Apprendre les maths derriÃ¨re (quaternions, projections sphÃ©riques)

**InadaptÃ© pour** :
- Production ou temps rÃ©el (trop lent)
- Traitement de gros volumes de vidÃ©os
- Tout cas d'usage oÃ¹ la performance compte

**La question maintenant** : peut-on garder la simplicitÃ© de Python tout en rattrapant FFmpeg ? La prochaine section explore la vectorisation NumPy - premiÃ¨re Ã©tape vers des performances acceptables sans quitter Python.

**Code complet** : [github.com/pykoder/fisheye-dewarping](lien-Ã -adapter)

## 2.3 NumPy vectorisÃ© - boost de performance majeur

### L'approche

On garde exactement le mÃªme algorithme que la version Python pur, mais on **Ã©limine toutes les boucles Python** en utilisant les opÃ©rations vectorisÃ©es de NumPy. L'idÃ©e : laisser NumPy (Ã©crit en C optimisÃ©) gÃ©rer les millions d'itÃ©rations au lieu de l'interprÃ©teur Python.

Le calcul du mapping reste identique (quaternions, matrice de projection), mais la phase d'application devient massivement parallÃ¨le grÃ¢ce au broadcasting et Ã  la vectorisation.

### Les modifications clÃ©s

Au lieu d'itÃ©rer pixel par pixel avec des boucles Python imbriquÃ©es, on calcule tout d'un coup en manipulant des tableaux entiers.

#### Phase 1 : Calcul du mapping vectorisÃ©

**Avant (Python pur)** :
```python
remap_zone = []
for j in range(self.output_height):
    line = []
    y = j - offset_height
    for i in range(self.output_width):
        x = i - offset_width
        # Calculs pour ce pixel...
        line.append((src_y, src_x))
    remap_zone.append(line)
```

**AprÃ¨s (NumPy vectorisÃ©)** :
```python
# CrÃ©er une grille de toutes les coordonnÃ©es d'un coup
i_coords, j_coords = np.meshgrid(
    np.arange(self.output_width),
    np.arange(self.output_height),
    indexing='xy')

# Aplatir et recentrer les coordonnÃ©es
x_coords = i_coords.flatten() * inv_width  - 1.0
y_coords = j_coords.flatten() * inv_height - 1.0

# Empiler en une matrice de coordonnÃ©es homogÃ¨nes
coords = np.column_stack([x_coords, y_coords, np.ones_like(x_coords)]).T

# UNE multiplication matricielle pour TOUS les pixels
xyz = R @ coords

# Calculs vectorisÃ©s (appliquÃ©s Ã  tous les pixels simultanÃ©ment)
hs = np.hypot(xyz[0, :],xyz[1, :])
phi = np.arctan2(hs, xyz[2, :])
coeff = phi / (hs * np.pi)
src_x = (self.width * (xyz[0, :] * coeff + 0.5)).astype(np.int32)
src_y = (self.height * (xyz[1, :] * coeff + 0.5)).astype(np.int32)

# Clipper pour rester dans les bornes de l'image
src_x = np.clip(src_x, 0, self.width - 1)
src_y = np.clip(src_y, 0, self.height - 1)

# Reshape en 2D et stocker
zone_mapping = np.stack([
    src_x.reshape((self.output_height, self.output_width)),
    src_y.reshape((self.output_height, self.output_width))
], axis=-1)
```

**Gain** : Au lieu de 960Ã—960 = 921,600 itÃ©rations de boucles Python, on a **une seule** multiplication matricielle optimisÃ©e en C + quelques opÃ©rations vectorisÃ©es. NumPy utilise les instructions SIMD du CPU (SSE, AVX) pour traiter plusieurs valeurs simultanÃ©ment.

#### Phase 2 : Application du mapping vectorisÃ©

**Avant (Python pur)** :
```python
for i in range(self.output_height):
    for j in range(self.output_width):
        try:
            output_buffer[i, j] = image[remap_table[i][j]]
        except IndexError:
            output_buffer[i, j] = [0, 0, 0]
```

**AprÃ¨s (NumPy vectorisÃ©)** :
```python
# Extraire les coordonnÃ©es sources
src_x = remap_table[:, :, 0]
src_y = remap_table[:, :, 1]

# Indexation avancÃ©e NumPy : copie TOUS les pixels d'un coup
output_buffer = image[src_y, src_x]
```

**DÃ©tail critique** : Le piÃ¨ge du masque de validitÃ©

Une premiÃ¨re version tentait de gÃ©rer explicitement les pixels hors-limites avec un masque boolÃ©en :
```python
valid_mask = ((src_y >= 0) & (src_y < self.height) &
              (src_x >= 0) & (src_x < self.width))
output_buffer[valid_mask] = image[src_y[valid_mask], src_x[valid_mask]]
```

**Impact dÃ©sastreux** : le benchmark passe de **7.97s Ã  18.58s** ! Pourquoi un simple masque de validitÃ© ralentit-il de 2.3Ã— ?

Le masque boolÃ©en casse la **localitÃ© mÃ©moire**. Sans masque, NumPy accÃ¨de aux pixels de faÃ§on relativement sÃ©quentielle, exploitant les caches CPU. Avec le masque, les accÃ¨s deviennent alÃ©atoires et dispersÃ©s - chaque pixel valide peut Ãªtre n'importe oÃ¹ dans l'image source. Le CPU passe son temps Ã  attendre des donnÃ©es du RAM au lieu de calculer.

**Solution Ã©lÃ©gante** : Clipper les coordonnÃ©es lors du calcul du mapping (Phase 1) :
```python
src_x = np.clip(src_x, 0, self.width - 1)
src_y = np.clip(src_y, 0, self.height - 1)
```

Les quelques pixels qui dÃ©passent pointent maintenant vers le bord de l'image (artefact visuel nÃ©gligeable sur quelques pixels) mais **tous les accÃ¨s mÃ©moire restent valides et sÃ©quentiels**. NumPy peut optimiser agressivement l'indexation.

### Benchmark
```
Commande: python3 unwarper_numpy.py -r 1024 ../images/fisheye.jpg

======================================================================
RESULTATS BENCHMARK
======================================================================
Wall time:              110.11s
CPU time (user+sys):     113.83s
  - User time:           113.14s
  - System time:         0.69s
CPU utilization:        103%
Cores utilises:         ~1.0
Memoire pic:            255.73 MB (261868 KB)
======================================================================

Speedup parallele:      1.03x
```

**RÃ©sultats** : 110 secondes pour traiter 1024 frames Ã— 5 vues. Soit environ **21.5 ms par frame et par vue**.

**Performance comparÃ©e** :
- FFmpeg : 208s, , 1411.84s CPU (6.8 cores) â†’ **276 ms/frame/vue/CPU**
- Python pur : 1889.36s wall, 1889.34s CPU (1.0 core) â†’ **369 ms/frame/vue**
- NumPy vectorisÃ© : 110.11s â†’ **21.5 ms/frame/vue/CPU**

### Analyse

**Gains significatifs** :
- **17Ã— plus rapide** que Python pur
- **1.9Ã— plus rapide** que FFmpeg
- **12.4Ã— moins de CPU** utilisÃ© que FFmpeg


La vectorisation NumPy Ã©limine le coÃ»t catastrophique des boucles Python.

**La magie de NumPy (partielle)** :

âœ… **Code C optimisÃ©** : Les opÃ©rations NumPy sont implÃ©mentÃ©es en C hautement optimisÃ© avec `-O3`.

âœ… **Vectorisation SIMD limitÃ©e** : Les calculs mathÃ©matiques (`hypot`, `arctan2`) exploitent les instructions AVX pour traiter 4-8 float64 simultanÃ©ment. Gain rÃ©el sur la Phase 1.

âœ… **LocalitÃ© mÃ©moire** : Les opÃ©rations vectorisÃ©es accÃ¨dent Ã  la mÃ©moire sÃ©quentiellement, maximisant l'efficacitÃ© du cache (sauf si on utilise le masque de validitÃ© !).

âŒ **ParallÃ©lisme limitÃ©** : NumPy ne parallÃ©lise que les grosses multiplications matricielles. L'indexation avancÃ©e reste mono-thread. Contrairement Ã  FFmpeg qui parallÃ©lise le dÃ©codage vidÃ©o + les filtres sur tous les cores.

âŒ **GIL partiellement prÃ©sent** : Certaines opÃ©rations NumPy relÃ¢chent le GIL, d'autres non. L'indexation avancÃ©e garde souvent le GIL, limitant le parallÃ©lisme.

**âœ… Points forts**

- **Performance acceptable** : 1.9Ã— plus rapide que FFmpeg, 17Ã— plus rapide que Python pur. Utilisable en production pour des volumes modÃ©rÃ©s.
- **Code toujours en Python** : GardÃ© la simplicitÃ© et la lisibilitÃ© de Python. Facile Ã  modifier, debugger, intÃ©grer dans un pipeline existant.
- **Pas de compilation** : Aucun toolchain C++, CMake ou dÃ©pendances systÃ¨me complexes. Juste `pip install numpy` et Ã§a tourne.
- **MÃ©moire optimisÃ©e** : 256 MB vs 638 MB (Python pur) et 1761 MB (FFmpeg). La table de mapping NumPy est compacte (array dense contiguÃ« en mÃ©moire).

**âŒ Points faibles**

- **ParallÃ©lisme dÃ©cevant** : 1 core vs 6.8 pour FFmpeg. On n'exploite pas le potentiel multi-core de la machine. L'indexation avancÃ©e reste le goulot mono-thread. Mais dans notre cas d'usage ce n'est pas un inconvÃ©nient.
- **Pas de gain Ã©norme sur FFmpeg** : 1.9Ã— plus rapide, c'est bien mais pas spectaculaire. FFmpeg reste compÃ©titif grÃ¢ce Ã  son parallÃ©lisme agressif.
- **DÃ©pendance NumPy** : NÃ©cessite NumPy + ses backends (OpenBLAS ou MKL). Packaging plus lourd qu'un simple script Python, possibles conflits de versions.
- **Courbe d'apprentissage** : Broadcasting, indexation avancÃ©e, piÃ¨ges de performance (masque de validitÃ©) - il faut maÃ®triser ces concepts pour ne pas se tirer une balle dans le pied.
- **Optimisations limitÃ©es** : On ne peut pas tweaker finement les stratÃ©gies d'accÃ¨s mÃ©moire ou le threading. NumPy dÃ©cide pour nous.

### Verdict

NumPy vectorisÃ© apporte un **gain substantiel de 17Ã— sur Python pur**, prouvant que la vectorisation fonctionne. Mais le gain modeste sur FFmpeg (1.4Ã—) rÃ©vÃ¨le les limites de cette approche : **on reste fondamentalement mono-thread** sur la partie critique (indexation).

**Cette implÃ©mentation est adaptÃ©e pour** :
- Pipelines Python existants oÃ¹ ajouter du C++ serait compliquÃ©
- Prototypage rapide avec performances acceptables
- Situations oÃ¹ 1.4Ã— plus rapide que FFmpeg suffit et oÃ¹ la simplicitÃ© de dÃ©ploiement prime

**Limitations** :
- Pour exploiter vraiment le multi-core, il faudrait sortir de Python
- L'indexation avancÃ©e NumPy ne rÃ©partit pas les traitements entre plusieurs coeurs, mais utilise seulement le SIMD.

**Peut-on faire mieux en restant en Python ?** Oui - la prochaine section explore OpenCV Python, qui offre des fonctions dÃ©diÃ©es au dewarping fisheye avec des optimisations spÃ©cifiques au traitement d'image.

**Code complet** : [github.com/pykoder/fisheye-dewarping](Code Source)

---

**Encore plus vite ?** La seconde partie de cet article explore trois implÃ©mentations supplÃ©mentaires : OpenCV Python (en utilisant la primitive `cv2.remap()`), OpenCV C++ (code OpenCV natif compilÃ©), pour finir par une bibliothÃ¨que C++ personnalisÃ©e qui offre des performances **42Ã— plus rapide** que FFmpeg. Comment ? A lire dans la seconde partie !

---

## Quoi de neuf dans la partie 2 2

Dans le prochain article nous explorons trois autres implÃ©mentations qui poussent le gain en performances toujours plus loin.

1. **OpenCV Python**: en utilisant `cv2.remap()` avec du parallÃ¨lisme multi-core (~4.8 cores)
2. **OpenCV C++**: code openCV natif compilÃ©, pour Ã©liminer l'overhead Python.
3. **BibliothÃ¨que C++ personnalisÃ©e**: l'optimisation ultime - 42Ã— plus rapide que FFmpeg en consommant seulement 80 MB de RAM

Nous verrons:
- Comment le multi-threading d'OpenCV permet de dÃ©passer l'efficacitÃ© mono-core de NumPy's
- Si du code C++ natif permet des gains significatifs par rapport Ã  Python
- Quelles optimisation permettent Ã  une bibliothÃ¨que C++ ad-hoc d'atteindre des performances de 4,91s pour 5120 vues. (0.96 ms/vue !)

**Points Ã  retenir de la partie 1**:
- FFmpeg: rapide mais coÃ»teux en mÃ©moire (1.78 GB), forte consommation CPU (1411s)
- Pur Python: lent mais efficace dans l'utilisation du CPU (seulement 1.3Ã— pire que FFmpeg par core)
- NumPy: Champion de l'efficacitÃ© - 12.4Ã— moins d'utilisation du CPU que FFmpeg, 7Ã— moins de mÃ©moire

**Spoiler alert**: The custom C++ library will process the same workload in just **4.91 seconds** using **1.1 cores** and **80 MB RAM**. That's:
- **42.5Ã— faster** than FFmpeg in wall time
- **256Ã— less CPU** consumption than FFmpeg
- **22Ã— less memory** than FFmpeg

How is this possible? Find out in Part 2!


**Full code for all implementations**: [github.com/pykoder/fisheye-dewarping](https://github.com/pykoder/fisheye-dewarping)

*Article written in December 2025. Benchmarks performed on a Lenovo ThinkPad P14s - Ubuntu 25.04, Intel Core i7-1185G7 (4 physical cores, 8 threads), 16GB RAM. All tests process 1024 frames Ã— 5 views = 5,120 dewarped images.*